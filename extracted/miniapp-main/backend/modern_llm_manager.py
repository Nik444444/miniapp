import os
import asyncio
import logging
import mimetypes
from typing import Dict, Any, Optional, Tuple
from abc import ABC, abstractmethod
import tempfile
import base64
from PIL import Image
import google.generativeai as genai
import openai
from anthropic import Anthropic

# Try to import emergentintegrations with fallback
try:
    from emergentintegrations.llm.chat import LlmChat, UserMessage, FileContentWithMimeType, ImageContent
    EMERGENT_INTEGRATIONS_AVAILABLE = True
except ImportError as e:
    logger = logging.getLogger(__name__)
    logger.warning(f"emergentintegrations not available: {e}")
    EMERGENT_INTEGRATIONS_AVAILABLE = False
    # Define fallback classes
    class LlmChat:
        def __init__(self, *args, **kwargs):
            pass
        async def send_message(self, *args, **kwargs):
            raise RuntimeError("emergentintegrations not available")
    class UserMessage:
        def __init__(self, *args, **kwargs):
            pass
    class FileContentWithMimeType:
        def __init__(self, *args, **kwargs):
            pass
    class ImageContent:
        def __init__(self, *args, **kwargs):
            pass

logger = logging.getLogger(__name__)

class ModernLLMProvider(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM"""

    def __init__(self, api_key: str, model_name: str):
        self.api_key = api_key
        self.model_name = model_name
        self.name = self.__class__.__name__

    @abstractmethod
    async def generate_content(self, prompt: str, image_path: Optional[str] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        pass

class ModernGeminiProvider(ModernLLMProvider):
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Google Gemini —á–µ—Ä–µ–∑ emergentintegrations"""

    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash"):
        super().__init__(api_key, model_name)
        self.session_id = f"gemini_session_{hash(api_key)}"

    async def generate_content(self, prompt: str, image_path: Optional[str] = None) -> str:
        try:
            if not EMERGENT_INTEGRATIONS_AVAILABLE:
                # Fallback mode - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                logger.warning("emergentintegrations not available - using fallback mode")
                fallback_message = (
                    "‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. "
                    "–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å emergentintegrations. "
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã."
                )
                if image_path:
                    fallback_message += "\n\nüìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ–∫—É—â–µ–º —Ä–µ–∂–∏–º–µ."
                return fallback_message
                
            if not self.api_key:
                raise Exception("Gemini API key not configured")

            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —á–∞—Ç–∞
            chat = LlmChat(
                api_key=self.api_key,
                session_id=self.session_id,
                system_message="You are a precise document analyzer. Extract ONLY factual information from documents. IMPORTANT: Always respond in the language requested in the user's prompt. Do NOT translate the document content, but DO respond in the requested language. If the prompt asks for Russian, respond in Russian. If it asks for English, respond in English. If it asks for German, respond in German. DO NOT interpret, assume, or add information that is not explicitly written in the text."
            ).with_model("gemini", self.model_name)

            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_message = UserMessage(text=prompt)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º FileContentWithMimeType –¥–ª—è Gemini)
            if image_path:
                with open(image_path, 'rb') as file:
                    file_content = file.read()
                    mime_type = mimetypes.guess_type(image_path)[0] or 'image/jpeg'
                    file_content_obj = FileContentWithMimeType(content=file_content, mime_type=mime_type)
                    user_message.attachments = [file_content_obj]

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
            response = await chat.send_message(user_message)
            
            logger.info(f"Modern Gemini response length: {len(response)}")
            return response

        except Exception as e:
            logger.error(f"Modern Gemini generation error: {e}")
            if "emergentintegrations not available" in str(e):
                return (
                    "‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. "
                    "–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å emergentintegrations. "
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã."
                )
            raise Exception(f"Gemini error: {str(e)}")

    def is_available(self) -> bool:
        return bool(self.api_key)

class ModernOpenAIProvider(ModernLLMProvider):
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è OpenAI —á–µ—Ä–µ–∑ emergentintegrations"""

    def __init__(self, api_key: str, model_name: str = "gpt-4o"):
        super().__init__(api_key, model_name)
        self.session_id = f"openai_session_{hash(api_key)}"

    async def generate_content(self, prompt: str, image_path: Optional[str] = None) -> str:
        try:
            if not EMERGENT_INTEGRATIONS_AVAILABLE:
                # Fallback mode - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                logger.warning("emergentintegrations not available - using fallback mode")
                fallback_message = (
                    "‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. "
                    "–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å emergentintegrations. "
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã."
                )
                if image_path:
                    fallback_message += "\n\nüìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ–∫—É—â–µ–º —Ä–µ–∂–∏–º–µ."
                return fallback_message
                
            if not self.api_key:
                raise Exception("OpenAI API key not configured")

            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —á–∞—Ç–∞
            chat = LlmChat(
                api_key=self.api_key,
                session_id=self.session_id,
                system_message="You are a precise document analyzer. Extract ONLY factual information from documents. IMPORTANT: Always respond in the language requested in the user's prompt. Do NOT translate the document content, but DO respond in the requested language. If the prompt asks for Russian, respond in Russian. If it asks for English, respond in English. If it asks for German, respond in German. DO NOT interpret, assume, or add information that is not explicitly written in the text."
            ).with_model("openai", self.model_name)

            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_message = UserMessage(text=prompt)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º base64 –¥–ª—è OpenAI)
            if image_path:
                with open(image_path, 'rb') as file:
                    file_content = file.read()
                    base64_content = base64.b64encode(file_content).decode('utf-8')
                    mime_type = mimetypes.guess_type(image_path)[0] or 'image/jpeg'
                    image_content = ImageContent(base64_content=base64_content, mime_type=mime_type)
                    user_message.attachments = [image_content]

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
            response = await chat.send_message(user_message)
            
            logger.info(f"Modern OpenAI response length: {len(response)}")
            return response

        except Exception as e:
            logger.error(f"Modern OpenAI generation error: {e}")
            if "emergentintegrations not available" in str(e):
                return (
                    "‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. "
                    "–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å emergentintegrations. "
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã."
                )
            raise Exception(f"OpenAI error: {str(e)}")

    def is_available(self) -> bool:
        return bool(self.api_key)

class ModernAnthropicProvider(ModernLLMProvider):
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Anthropic —á–µ—Ä–µ–∑ emergentintegrations"""

    def __init__(self, api_key: str, model_name: str = "claude-3-5-sonnet-20241022"):
        super().__init__(api_key, model_name)
        self.session_id = f"anthropic_session_{hash(api_key)}"

    async def generate_content(self, prompt: str, image_path: Optional[str] = None) -> str:
        try:
            if not EMERGENT_INTEGRATIONS_AVAILABLE:
                # Fallback mode - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                logger.warning("emergentintegrations not available - using fallback mode")
                fallback_message = (
                    "‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. "
                    "–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å emergentintegrations. "
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã."
                )
                if image_path:
                    fallback_message += "\n\nüìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ–∫—É—â–µ–º —Ä–µ–∂–∏–º–µ."
                return fallback_message
                
            if not self.api_key:
                raise Exception("Anthropic API key not configured")

            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —á–∞—Ç–∞
            chat = LlmChat(
                api_key=self.api_key,
                session_id=self.session_id,
                system_message="You are a precise document analyzer. Extract ONLY factual information from documents. IMPORTANT: Always respond in the language requested in the user's prompt. Do NOT translate the document content, but DO respond in the requested language. If the prompt asks for Russian, respond in Russian. If it asks for English, respond in English. If it asks for German, respond in German. DO NOT interpret, assume, or add information that is not explicitly written in the text."
            ).with_model("anthropic", self.model_name)

            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_message = UserMessage(text=prompt)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º base64 –¥–ª—è Anthropic)
            if image_path:
                with open(image_path, 'rb') as file:
                    file_content = file.read()
                    base64_content = base64.b64encode(file_content).decode('utf-8')
                    mime_type = mimetypes.guess_type(image_path)[0] or 'image/jpeg'
                    image_content = ImageContent(base64_content=base64_content, mime_type=mime_type)
                    user_message.attachments = [image_content]

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
            response = await chat.send_message(user_message)
            
            logger.info(f"Modern Anthropic response length: {len(response)}")
            return response

        except Exception as e:
            logger.error(f"Modern Anthropic generation error: {e}")
            if "emergentintegrations not available" in str(e):
                return (
                    "‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏. "
                    "–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å emergentintegrations. "
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã."
                )
            raise Exception(f"Anthropic error: {str(e)}")

    def is_available(self) -> bool:
        return bool(self.api_key)

class ModernLLMManager:
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ LLM"""

    def __init__(self):
        self.providers: Dict[str, ModernLLMProvider] = {}
        self.initialize_providers()

    def initialize_providers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        try:
            # Gemini
            gemini_key = os.environ.get('GEMINI_API_KEY')
            if gemini_key:
                self.providers['gemini'] = ModernGeminiProvider(gemini_key)

            # OpenAI
            openai_key = os.environ.get('OPENAI_API_KEY')
            if openai_key:
                self.providers['openai'] = ModernOpenAIProvider(openai_key)

            # Anthropic
            anthropic_key = os.environ.get('ANTHROPIC_API_KEY')
            if anthropic_key:
                self.providers['anthropic'] = ModernAnthropicProvider(anthropic_key)

            logger.info(f"Modern LLM Manager initialized with {len(self.providers)} providers")
        except Exception as e:
            logger.error(f"Failed to initialize modern providers: {e}")

    def get_provider_status(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        status = {}
        for name, provider in self.providers.items():
            status[name] = {
                "status": "active" if provider.is_available() else "inactive",
                "model": provider.model_name,
                "name": provider.name,
                "modern": True
            }

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –±–µ–∑ –∫–ª—é—á–µ–π
        all_providers = ['gemini', 'openai', 'anthropic']
        for provider_name in all_providers:
            if provider_name not in status:
                status[provider_name] = {
                    "status": "inactive",
                    "model": "N/A",
                    "name": f"Modern{provider_name.title()}Provider",
                    "modern": True
                }

        return status

    def create_user_provider(self, provider_type: str, model_name: str, api_key: str) -> ModernLLMProvider:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º API –∫–ª—é—á–æ–º"""
        if provider_type.lower() == 'gemini':
            return ModernGeminiProvider(api_key, model_name)
        elif provider_type.lower() == 'openai':
            return ModernOpenAIProvider(api_key, model_name)
        elif provider_type.lower() == 'anthropic':
            return ModernAnthropicProvider(api_key, model_name)
        else:
            raise ValueError(f"Unsupported provider type: {provider_type}")

    async def test_api_key(self, provider_type: str, api_key: str) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API –∫–ª—é—á–∞"""
        try:
            # –ï—Å–ª–∏ emergentintegrations –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            if not EMERGENT_INTEGRATIONS_AVAILABLE:
                logger.warning(f"emergentintegrations not available, using fallback API key validation for {provider_type}")
                
                # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ API –∫–ª—é—á–∞
                if provider_type.lower() == 'gemini':
                    # Gemini API keys –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 'AIza'
                    if api_key and api_key.startswith('AIza') and len(api_key) > 30:
                        logger.info(f"Gemini API key format appears valid (fallback validation)")
                        return True
                    else:
                        logger.warning(f"Gemini API key format invalid: {api_key[:10]}...")
                        return False
                elif provider_type.lower() == 'openai':
                    # OpenAI API keys –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 'sk-'
                    if api_key and api_key.startswith('sk-') and len(api_key) > 40:
                        logger.info(f"OpenAI API key format appears valid (fallback validation)")
                        return True
                    else:
                        logger.warning(f"OpenAI API key format invalid")
                        return False
                elif provider_type.lower() == 'anthropic':
                    # Anthropic API keys –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 'sk-ant-'
                    if api_key and api_key.startswith('sk-ant-') and len(api_key) > 40:
                        logger.info(f"Anthropic API key format appears valid (fallback validation)")
                        return True
                    else:
                        logger.warning(f"Anthropic API key format invalid")
                        return False
                else:
                    return False
            
            # –ï—Å–ª–∏ emergentintegrations –¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            if provider_type.lower() == 'gemini':
                provider = ModernGeminiProvider(api_key)
            elif provider_type.lower() == 'openai':
                provider = ModernOpenAIProvider(api_key)
            elif provider_type.lower() == 'anthropic':
                provider = ModernAnthropicProvider(api_key)
            else:
                return False

            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –ø—Ä–æ—Å—Ç—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
            response = await provider.generate_content("Test message")
            return bool(response)
        except Exception as e:
            logger.error(f"API key test failed for {provider_type}: {e}")
            return False

    async def generate_content(
        self, 
        prompt: str, 
        image_path: Optional[str] = None,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö API –∫–ª—é—á–µ–π"""
        
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏ API –∫–ª—é—á, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if provider and api_key:
            try:
                logger.info(f"Using user provider: {provider} with custom API key")
                user_provider = self.create_user_provider(provider, model or self._get_default_model(provider), api_key)
                response = await user_provider.generate_content(prompt, image_path)
                if response:
                    logger.info(f"User provider {provider} successful")
                    return response
                else:
                    logger.warning(f"User provider {provider} returned empty response")
            except Exception as e:
                logger.error(f"User provider {provider} failed: {e}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é –æ—à–∏–±–∫—É –æ –ø—Ä–æ–±–ª–µ–º–µ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º API –∫–ª—é—á–æ–º
                return "AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –≤–∞—à–µ–≥–æ API –∫–ª—é—á–∞ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –ø—Ä–æ—Ñ–∏–ª—è."

        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        active_providers = [name for name, provider in self.providers.items() if provider.is_available()]

        if not active_providers:
            return "–î–µ–º–æ –∞–Ω–∞–ª–∏–∑: –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –¥–µ–º–æ-—Ä–µ–∂–∏–º–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ API –∫–ª—é—á –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏."

        # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        priority_order = ['gemini', 'openai', 'anthropic']

        for provider_name in priority_order:
            if provider_name in active_providers:
                try:
                    provider_obj = self.providers[provider_name]
                    response = await provider_obj.generate_content(prompt, image_path)
                    return response
                except Exception as e:
                    logger.warning(f"Modern provider {provider_name} failed: {e}")
                    continue

        return "AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á–∏ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."

    def _get_default_model(self, provider: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        defaults = {
            'gemini': 'gemini-2.0-flash',
            'openai': 'gpt-4o',
            'anthropic': 'claude-3-5-sonnet-20241022'
        }
        return defaults.get(provider, 'default')

    def get_available_providers(self) -> Dict[str, bool]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        return {name: provider.is_available() for name, provider in self.providers.items()}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
modern_llm_manager = ModernLLMManager()