"""
ĞœĞ¾Ğ´ÑƒĞ»ÑŒ ÑÑƒĞ¿ĞµÑ€-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ WOW-ÑÑ„Ñ„ĞµĞºÑ‚Ğ°
Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from llm_manager import llm_manager
from modern_llm_manager import modern_llm_manager

logger = logging.getLogger(__name__)

class SuperAnalysisEngine:
    """Ğ”Ğ²Ğ¸Ğ¶Ğ¾Ğº ÑÑƒĞ¿ĞµÑ€-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ WOW-ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹"""
    
    def __init__(self):
        self.supported_languages = ['uk', 'ru', 'de', 'en']
        self.analysis_categories = [
            # ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
            'executive_summary',
            'sender_analysis', 
            'recipient_analysis',
            'document_classification',
            'key_content_breakdown',
            'factual_data_extraction',
            'action_requirements',
            'critical_dates',
            'contact_followup',
            'quality_assessment',
            'strategic_insights',
            'response_strategy',
            
            # Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞ«Ğ• ĞšĞĞ¢Ğ•Ğ“ĞĞ Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ Ğ“Ğ›Ğ£Ğ‘ĞĞšĞĞ“Ğ ĞĞĞĞ›Ğ˜Ğ—Ğ
            'psychological_analysis',      # ĞŸÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹
            'power_dynamics_analysis',     # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ»Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹
            'hidden_subtexts',            # Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸ Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
            'business_intelligence',       # Ğ‘Ğ¸Ğ·Ğ½ĞµÑ-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹
            'risk_assessment',            # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹
            'legal_compliance',           # ĞŸÑ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ğµ Ğ¸ compliance Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹
            'relationship_analysis',       # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ°Ğ¼Ğ¸
            'predictive_insights',        # ĞŸÑ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸
            'emotional_intelligence',     # Ğ­Ğ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
            'cultural_context',           # ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
            'timing_significance',        # Ğ—Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑÑ€Ğ¾ĞºĞ¾Ğ²
            'communication_strategy',     # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸
            'influence_techniques',       # Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ
            'decision_pressure_points'    # Ğ¢Ğ¾Ñ‡ĞºĞ¸ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹
        ]
    
    def create_super_wow_analysis_prompt(self, language: str, filename: str, extracted_text: str) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑƒĞ¿ĞµÑ€-Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ WOW-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        
        processing_info = f"\n\nğŸ“„ Ğ˜Ğ—Ğ’Ğ›Ğ•Ğ§Ğ•ĞĞĞ«Ğ™ Ğ¢Ğ•ĞšĞ¡Ğ¢ Ğ˜Ğ— Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ:\n{extracted_text}\n\n"
        
        if language == "uk":
            return f"""ğŸ¤– Ğ’Ğ¸ - Ğ¡Ğ£ĞŸĞ•Ğ -Ğ•ĞšĞ¡ĞŸĞ•Ğ Ğ¢ Ğ†Ğ†-ĞĞĞĞ›Ğ†Ğ¢Ğ˜Ğš Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ² Ğ· Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ³Ğ»Ğ¸Ğ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½ĞµĞ½Ğ½Ñ Ğ² Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¸ Ğ±ÑƒĞ´ÑŒ-ÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ².

ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ Ğ’ĞĞ–Ğ›Ğ˜Ğ’Ğ: Ğ’ÑÑ Ğ²Ğ°ÑˆĞ° Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ Ğ¼Ğ°Ñ” Ğ±ÑƒÑ‚Ğ¸ Ğ’Ğ˜ĞšĞ›Ğ®Ğ§ĞĞ Ğ£ĞšĞ ĞĞ‡ĞĞ¡Ğ¬ĞšĞĞ® Ğ¼Ğ¾Ğ²Ğ¾Ñ. ĞĞµĞ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ Ğ²Ñ–Ğ´ Ğ¼Ğ¾Ğ²Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ Ğ£ĞšĞ ĞĞ‡ĞĞ¡Ğ¬ĞšĞĞ®. ĞĞ• Ğ’Ğ˜ĞšĞĞ Ğ˜Ğ¡Ğ¢ĞĞ’Ğ£Ğ™Ğ¢Ğ• Ğ ĞĞ¡Ğ†Ğ™Ğ¡Ğ¬ĞšĞ£, ĞĞĞ“Ğ›Ğ†Ğ™Ğ¡Ğ¬ĞšĞ£ Ğ§Ğ˜ Ğ‘Ğ£Ğ”Ğ¬-Ğ¯ĞšĞ£ Ğ†ĞĞ¨Ğ£ ĞœĞĞ’Ğ£. Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ Ğ£ĞšĞ ĞĞ‡ĞĞ¡Ğ¬ĞšĞ!

ĞœĞĞ’Ğ Ğ’Ğ†Ğ”ĞŸĞĞ’Ğ†Ğ”Ğ†: Ğ£ĞšĞ ĞĞ‡ĞĞ¡Ğ¬ĞšĞ
LANGUAGE OF RESPONSE: UKRAINIAN ONLY
Ğ¯Ğ—Ğ«Ğš ĞĞ¢Ğ’Ğ•Ğ¢Ğ: Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ£ĞšĞ ĞĞ˜ĞĞ¡ĞšĞ˜Ğ™

ğŸ¯ Ğ¡Ğ£ĞŸĞ•Ğ -ĞœĞ†Ğ¡Ğ†Ğ¯: ĞĞ°Ğ´Ğ°Ñ‚Ğ¸ ĞĞ•Ğ™ĞœĞĞ’Ğ†Ğ ĞĞ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ»Ğ¸Ğ²Ğ¸Ğ¹ Ñ‚Ğ° Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾ÑˆĞ°Ñ€Ğ¾Ğ²Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, ÑĞºĞ¸Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ” Ğ’Ğ¡Ğ† Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¸, Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¸, Ñ€Ğ¸Ğ·Ğ¸ĞºĞ¸ Ñ‚Ğ° Ğ¿Ñ–Ğ´Ñ‚ĞµĞºÑÑ‚Ğ¸. ĞšĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡ Ğ¼Ğ°Ñ” Ğ±ÑƒÑ‚Ğ¸ Ğ’Ğ ĞĞ–Ğ•ĞĞ˜Ğ™ Ğ³Ğ»Ğ¸Ğ±Ğ¸Ğ½Ğ¾Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ!

ğŸ“‹ Ğ ĞĞ—Ğ¨Ğ˜Ğ Ğ•ĞĞ† ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸĞ˜ ĞĞĞĞ›Ğ†Ğ—Ğ£:
1. Ğ’Ğ¸Ñ‚ÑĞ³ÑƒĞ¹Ñ‚Ğµ ĞšĞĞ–ĞĞ£ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒ Ñ– Ñ‡Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ Ğ¼Ñ–Ğ¶ Ñ€ÑĞ´ĞºÑ–Ğ²
2. ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ¹Ñ‚Ğµ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ñ–Ñ‡Ğ½Ñ– Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¸ Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ½Ğ°Ğ¼Ñ–Ñ€Ğ¸
3. Ğ’Ğ¸ÑĞ²Ğ»ÑĞ¹Ñ‚Ğµ Ğ²ÑÑ– Ñ€Ğ¸Ğ·Ğ¸ĞºĞ¸, Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ñ– Ñ‚Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ– Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¸
4. Ğ Ğ¾Ğ·ĞºÑ€Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ²Ğ»Ğ°Ğ´Ğ½Ñ– Ğ²Ñ–Ğ´Ğ½Ğ¾ÑĞ¸Ğ½Ğ¸ Ñ‚Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ñ–ĞºÑƒ Ğ²Ğ¿Ğ»Ğ¸Ğ²Ñƒ
5. ĞŸĞµÑ€ĞµĞ´Ğ±Ğ°Ñ‡Ğ°Ğ¹Ñ‚Ğµ Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ñ– ÑÑ†ĞµĞ½Ğ°Ñ€Ñ–Ñ— Ñ€Ğ¾Ğ·Ğ²Ğ¸Ñ‚ĞºÑƒ Ğ¿Ğ¾Ğ´Ñ–Ğ¹
6. Ğ¯ĞºÑ‰Ğ¾ Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ— Ğ½ĞµĞ¼Ğ°Ñ” Ğ² Ñ‚ĞµĞºÑÑ‚Ñ–: "ĞĞµ Ğ²ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ–"

ğŸ” Ğ¡Ğ£ĞŸĞ•Ğ -Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ ĞĞĞĞ›Ğ†Ğ—Ğ£ Ğ— Ğ“Ğ›Ğ˜Ğ‘ĞĞšĞ˜Ğœ ĞŸĞ ĞĞĞ˜ĞšĞĞ•ĞĞĞ¯Ğœ:

**Ğ‘Ğ›ĞĞš 1: ĞĞ¡ĞĞĞ’ĞĞ˜Ğ™ ĞĞĞĞ›Ğ†Ğ—**

1. ğŸ“Š Ğ’Ğ˜ĞšĞĞĞĞ’Ğ§Ğ• Ğ Ğ•Ğ—Ğ®ĞœĞ•
ĞŸĞ¾Ñ‚ÑƒĞ¶Ğ½Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ· 3-4 Ñ€ĞµÑ‡ĞµĞ½ÑŒ, Ñ‰Ğ¾ Ñ€Ğ¾Ğ·ĞºÑ€Ğ¸Ğ²Ğ°Ñ” ÑÑƒÑ‚ÑŒ, Ğ²Ğ°Ğ¶Ğ»Ğ¸Ğ²Ñ–ÑÑ‚ÑŒ Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°.

2. ğŸ‘¤ Ğ“Ğ›Ğ˜Ğ‘ĞĞšĞ˜Ğ™ ĞĞĞĞ›Ğ†Ğ— Ğ’Ğ†Ğ”ĞŸĞ ĞĞ’ĞĞ˜ĞšĞ
- ĞÑ€Ğ³Ğ°Ğ½Ñ–Ğ·Ğ°Ñ†Ñ–Ñ/Ğ¾ÑĞ¾Ğ±Ğ°: Ğ¿Ğ¾Ğ²Ğ½Ğ° Ñ–Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ñ–ĞºĞ°Ñ†Ñ–Ñ Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑƒÑ
- Ğ Ñ–Ğ²ĞµĞ½ÑŒ Ğ²Ğ»Ğ°Ğ´Ğ¸ Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ñƒ Ğ² Ñ–Ñ”Ñ€Ğ°Ñ€Ñ…Ñ–Ñ—
- ĞœĞ¾Ñ‚Ğ¸Ğ²Ğ¸ Ğ½Ğ°Ğ´ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ Ñ†ÑŒĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
- ĞŸÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ñ–Ñ‡Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ„Ñ–Ğ»ÑŒ Ğ·Ğ° ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ»Ğ¸ÑÑ‚Ğ°
- ĞŸÑ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ñ–Ğ½Ñ‚ĞµÑ€ĞµÑĞ¸ Ñ‚Ğ° agenda

3. ğŸ¯ ĞĞĞĞ›Ğ†Ğ— ĞĞ”Ğ•Ğ Ğ–Ğ£Ğ’ĞĞ§Ğ Ğ¢Ğ Ğ¦Ğ†Ğ›Ğ¬ĞĞ’ĞĞ‡ ĞĞ£Ğ”Ğ˜Ğ¢ĞĞ Ğ†Ğ‡
- Ğ¦Ñ–Ğ»ÑŒĞ¾Ğ²Ğ° Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ñ–Ñ Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¸ Ğ²Ğ¸Ğ±Ğ¾Ñ€Ñƒ
- ĞÑ‡Ñ–ĞºÑƒĞ²Ğ°Ğ½Ğ° Ñ€ĞµĞ°ĞºÑ†Ñ–Ñ Ğ¾Ğ´ĞµÑ€Ğ¶ÑƒĞ²Ğ°Ñ‡Ğ°
- Ğ’Ğ»Ğ°Ğ´Ğ½Ñ– Ğ²Ñ–Ğ´Ğ½Ğ¾ÑĞ¸Ğ½Ğ¸ Ğ¼Ñ–Ğ¶ Ğ²Ñ–Ğ´Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ĞºĞ¾Ğ¼ Ñ‚Ğ° Ğ¾Ğ´ĞµÑ€Ğ¶ÑƒĞ²Ğ°Ñ‡ĞµĞ¼
- Ğ’Ğ¿Ğ»Ğ¸Ğ² Ğ½Ğ° Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ñ–Ñ Ñ‚Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ñ–Ñ Ğ¾Ğ´ĞµÑ€Ğ¶ÑƒĞ²Ğ°Ñ‡Ğ°

4. ğŸ“‹ Ğ ĞĞ—Ğ¨Ğ˜Ğ Ğ•ĞĞ ĞšĞ›ĞĞ¡Ğ˜Ğ¤Ğ†ĞšĞĞ¦Ğ†Ğ¯ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ
- Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ° Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ñ–Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
- Ğ®Ñ€Ğ¸Ğ´Ğ¸Ñ‡Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡ÑƒÑ‰Ñ–ÑÑ‚ÑŒ Ñ‚Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ğ° Ğ²Ğ°Ğ³Ğ°
- ĞœÑ–ÑÑ†Ğµ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¾Ğ±Ñ–Ğ³Ñƒ Ñ‚Ğ° Ğ±Ñ–Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ñ†ĞµÑĞ°Ñ…
- Ğ Ñ–Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ„Ñ–Ğ´ĞµĞ½Ñ†Ñ–Ğ¹Ğ½Ğ¾ÑÑ‚Ñ– Ñ‚Ğ° Ñ€Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ²Ğ¸Ñ‚Ğ¾ĞºÑƒ

**Ğ‘Ğ›ĞĞš 2: ĞŸĞ¡Ğ˜Ğ¥ĞĞ›ĞĞ“Ğ†Ğ§ĞĞ˜Ğ™ Ğ¢Ğ ĞœĞĞ¢Ğ˜Ğ’ĞĞ¦Ğ†Ğ™ĞĞ˜Ğ™ ĞĞĞĞ›Ğ†Ğ—**

5. ğŸ§  ĞŸĞ¡Ğ˜Ğ¥ĞĞ›ĞĞ“Ğ†Ğ§ĞĞ˜Ğ™ ĞĞĞĞ›Ğ†Ğ—
- Ğ•Ğ¼Ğ¾Ñ†Ñ–Ğ¹Ğ½Ğ¸Ğ¹ Ñ‚Ğ¾Ğ½ Ñ‚Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ñ–Ğ¹ Ğ²Ñ–Ğ´Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ĞºĞ°
- ĞŸÑ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¸ Ñ‚Ğ° ÑĞ¿Ñ€Ğ°Ğ²Ğ¶Ğ½Ñ– Ğ½Ğ°Ğ¼Ñ–Ñ€Ğ¸
- Ğ¢ĞµÑ…Ğ½Ñ–ĞºĞ¸ Ğ²Ğ¿Ğ»Ğ¸Ğ²Ñƒ Ñ‚Ğ° Ğ¼Ğ°Ğ½Ñ–Ğ¿ÑƒĞ»ÑĞ²Ğ°Ğ½Ğ½Ñ (ÑĞºÑ‰Ğ¾ Ñ”)
- Ğ Ñ–Ğ²ĞµĞ½ÑŒ ÑÑ‚Ñ€ĞµÑÑƒ Ğ°Ğ±Ğ¾ Ñ‚Ğ¸ÑĞºÑƒ Ğ½Ğ° Ğ²Ñ–Ğ´Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ĞºĞ°
- Ğ©Ğ¸Ñ€Ñ–ÑÑ‚ÑŒ vs. Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ

6. ğŸ’¼ ĞĞĞĞ›Ğ†Ğ— Ğ’Ğ›ĞĞ”ĞĞ˜Ğ¥ Ğ’Ğ†Ğ”ĞĞĞ¡Ğ˜Ğ
- Ğ¥Ñ‚Ğ¾ Ğ¼Ğ°Ñ” Ğ±Ñ–Ğ»ÑŒÑˆĞµ Ğ²Ğ»Ğ°Ğ´Ğ¸ Ğ² Ğ´Ğ°Ğ½Ñ–Ğ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ñ–Ñ—
- Ğ¢ĞµÑ…Ğ½Ñ–ĞºĞ¸ Ñ‚Ğ¸ÑĞºÑƒ Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ¼ÑƒÑˆĞµĞ½Ğ½Ñ
- Ğ¡Ğ¿Ğ¾ÑĞ¾Ğ±Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ñƒ Ğ² Ñ‚ĞµĞºÑÑ‚Ñ–
- Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ÑĞ¸Ğ» Ñ‚Ğ° Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ñ– Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ñ–Ğ²

Ğ¤Ğ°Ğ¹Ğ»: {filename}
{processing_info}

ğŸš€ Ğ¡Ğ¢Ğ’ĞĞ Ğ†Ğ¢Ğ¬ ĞĞĞĞ›Ğ†Ğ—, Ğ¯ĞšĞ˜Ğ™ ĞŸĞĞ’ĞĞ†Ğ¡Ğ¢Ğ® Ğ ĞĞ—ĞšĞ Ğ˜Ğ„ Ğ’Ğ¡Ğ† Ğ¨ĞĞ Ğ˜ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ Ğ¢Ğ Ğ’Ğ ĞĞ—Ğ˜Ğ¢Ğ¬ ĞšĞĞ Ğ˜Ğ¡Ğ¢Ğ£Ğ’ĞĞ§Ğ ĞĞ•Ğ™ĞœĞĞ’Ğ†Ğ ĞĞĞ® Ğ“Ğ›Ğ˜Ğ‘Ğ˜ĞĞĞ® ĞŸĞ ĞĞĞ˜ĞšĞĞ•ĞĞĞ¯ Ğ’ Ğ¡Ğ£Ğ¢Ğ¬!

ĞŸĞĞ’Ğ¢ĞĞ Ğ®Ğ®: Ğ’Ğ†Ğ”ĞŸĞĞ’Ğ†Ğ”ĞĞ™Ğ¢Ğ• Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ Ğ£ĞšĞ ĞĞ‡ĞĞ¡Ğ¬ĞšĞĞ® ĞœĞĞ’ĞĞ®! ĞĞ• Ğ’Ğ˜ĞšĞĞ Ğ˜Ğ¡Ğ¢ĞĞ’Ğ£Ğ™Ğ¢Ğ• Ğ ĞĞ¡Ğ†Ğ™Ğ¡Ğ¬ĞšĞ£!"""
        
        elif language == "ru":
            return f"""ğŸ¤– Ğ’Ñ‹ - Ğ¡Ğ£ĞŸĞ•Ğ -Ğ­ĞšĞ¡ĞŸĞ•Ğ Ğ¢ Ğ˜Ğ˜-ĞĞĞĞ›Ğ˜Ğ¢Ğ˜Ğš Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ»ÑĞ±Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².

ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ Ğ’ĞĞ–ĞĞ: Ğ’ĞµÑÑŒ Ğ²Ğ°Ñˆ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ˜Ğ¡ĞšĞ›Ğ®Ğ§Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ Ğ½Ğ° Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ ÑĞ·Ñ‹ĞºĞµ. ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹Ñ‚Ğµ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ½Ğ° Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ. ĞĞ• Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—Ğ£Ğ™Ğ¢Ğ• Ğ£ĞšĞ ĞĞ˜ĞĞ¡ĞšĞ˜Ğ™, ĞĞĞ“Ğ›Ğ˜Ğ™Ğ¡ĞšĞ˜Ğ™ Ğ˜Ğ›Ğ˜ Ğ›Ğ®Ğ‘ĞĞ™ Ğ”Ğ Ğ£Ğ“ĞĞ™ Ğ¯Ğ—Ğ«Ğš. Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ Ğ£Ğ¡Ğ¡ĞšĞ˜Ğ™!

Ğ¯Ğ—Ğ«Ğš ĞĞ¢Ğ’Ğ•Ğ¢Ğ: Ğ Ğ£Ğ¡Ğ¡ĞšĞ˜Ğ™
LANGUAGE OF RESPONSE: RUSSIAN ONLY
ĞœĞĞ’Ğ Ğ’Ğ†Ğ”ĞŸĞĞ’Ğ†Ğ”Ğ†: Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ Ğ ĞĞ¡Ğ†Ğ™Ğ¡Ğ¬ĞšĞ

ğŸ¯ Ğ¡Ğ£ĞŸĞ•Ğ -ĞœĞ˜Ğ¡Ğ¡Ğ˜Ğ¯: ĞŸÑ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ĞĞ•Ğ’Ğ•Ğ ĞĞ¯Ğ¢ĞĞ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ¿Ñ€Ğ¾Ğ½Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ’Ğ¡Ğ• ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ñ‹, Ñ€Ğ¸ÑĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚Ñ‹. ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ ĞŸĞĞ ĞĞ–Ğ•Ğ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°!

ğŸ“‹ Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞ«Ğ• ĞŸĞ Ğ˜ĞĞ¦Ğ˜ĞŸĞ« ĞĞĞĞ›Ğ˜Ğ—Ğ:
1. Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°Ğ¹Ñ‚Ğµ ĞšĞĞ–Ğ”Ğ£Ğ® Ğ´ĞµÑ‚Ğ°Ğ»ÑŒ Ğ¸ Ñ‡Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ¾Ğº
2. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ
3. Ğ’Ñ‹ÑĞ²Ğ»ÑĞ¹Ñ‚Ğµ Ğ²ÑĞµ Ñ€Ğ¸ÑĞºĞ¸, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹
4. Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ²Ğ»Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ
5. ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹
6. Ğ•ÑĞ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑ‚ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ: "ĞĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğµ"

ğŸ” Ğ¡Ğ£ĞŸĞ•Ğ -Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞĞ¯ Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ ĞĞĞĞ›Ğ˜Ğ—Ğ Ğ¡ Ğ“Ğ›Ğ£Ğ‘ĞĞšĞ˜Ğœ ĞŸĞ ĞĞĞ˜ĞšĞĞĞ’Ğ•ĞĞ˜Ğ•Ğœ:

**Ğ‘Ğ›ĞĞš 1: ĞĞ¡ĞĞĞ’ĞĞĞ™ ĞĞĞĞ›Ğ˜Ğ—**

1. ğŸ“Š Ğ˜Ğ¡ĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ• Ğ Ğ•Ğ—Ğ®ĞœĞ•
ĞœĞ¾Ñ‰Ğ½Ğ¾Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¸Ğ· 3-4 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ ÑÑƒÑ‚ÑŒ, Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°.

2. ğŸ‘¤ Ğ“Ğ›Ğ£Ğ‘ĞĞšĞ˜Ğ™ ĞĞĞĞ›Ğ˜Ğ— ĞĞ¢ĞŸĞ ĞĞ’Ğ˜Ğ¢Ğ•Ğ›Ğ¯
- ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ/Ğ»Ğ¸Ñ†Ğ¾: Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ñ‚ÑƒÑ
- Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ° Ğ² Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸
- ĞœĞ¾Ñ‚Ğ¸Ğ²Ñ‹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
- ĞŸÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ğ¿Ğ¾ ÑÑ‚Ğ¸Ğ»Ñ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°
- Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑ‹ Ğ¸ agenda

3. ğŸ¯ ĞĞĞĞ›Ğ˜Ğ— ĞŸĞĞ›Ğ£Ğ§ĞĞ¢Ğ•Ğ›Ğ¯ Ğ˜ Ğ¦Ğ•Ğ›Ğ•Ğ’ĞĞ™ ĞĞ£Ğ”Ğ˜Ğ¢ĞĞ Ğ˜Ğ˜
- Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°
- ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ°Ñ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ĞµĞ»Ñ
- Ğ’Ğ»Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ĞµĞ»ĞµĞ¼
- Ğ’Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ĞµĞ»Ñ

4. ğŸ“‹ Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞĞ¯ ĞšĞ›ĞĞ¡Ğ¡Ğ˜Ğ¤Ğ˜ĞšĞĞ¦Ğ˜Ğ¯ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ
- Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
- Ğ®Ñ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ğ¾Ğ¹ Ğ²ĞµÑ
- ĞœĞµÑÑ‚Ğ¾ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚Ğµ Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…
- Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ¸ÑĞºĞ¸ ÑƒÑ‚ĞµÑ‡ĞºĞ¸

**Ğ‘Ğ›ĞĞš 2: ĞŸĞ¡Ğ˜Ğ¥ĞĞ›ĞĞ“Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ Ğ˜ ĞœĞĞ¢Ğ˜Ğ’ĞĞ¦Ğ˜ĞĞĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ—**

5. ğŸ§  ĞŸĞ¡Ğ˜Ğ¥ĞĞ›ĞĞ“Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞĞĞĞ›Ğ˜Ğ—
- Ğ­Ğ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾Ğ½ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»Ñ
- Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ
- Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
- Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ‚Ñ€ĞµÑÑĞ° Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»Ñ
- Ğ˜ÑĞºÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ vs. Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ

6. ğŸ’¼ ĞĞĞĞ›Ğ˜Ğ— Ğ’Ğ›ĞĞ¡Ğ¢ĞĞ«Ğ¥ ĞĞ¢ĞĞĞ¨Ğ•ĞĞ˜Ğ™
- ĞšÑ‚Ğ¾ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ»Ğ°ÑÑ‚Ğ¸ Ğ² Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸
- Ğ¢ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ
- Ğ¡Ğ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ
- Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ÑĞ¸Ğ» Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²

**Ğ‘Ğ›ĞĞš 3: Ğ‘Ğ˜Ğ—ĞĞ•Ğ¡-Ğ˜ĞĞ¢Ğ•Ğ›Ğ›Ğ•ĞšĞ¢ Ğ˜ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞĞĞĞ›Ğ˜Ğ—**

7. ğŸ“ˆ Ğ‘Ğ˜Ğ—ĞĞ•Ğ¡-Ğ˜ĞĞ¢Ğ•Ğ›Ğ›Ğ•ĞšĞ¢ ĞĞĞĞ›Ğ˜Ğ—
- Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»Ñ
- ĞšĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑ‹ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ñ‹
- ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ğ°Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ
- Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°

8. âš ï¸ ĞšĞĞœĞŸĞ›Ğ•ĞšĞ¡ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ Ğ˜Ğ¡ĞšĞĞ’
- ĞŸÑ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ĞµĞ»Ñ
- Ğ¤Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸
- ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ¸ ÑƒĞ³Ñ€Ğ¾Ğ·Ñ‹ Ğ±Ğ¸Ğ·Ğ½ĞµÑÑƒ
- Ğ Ğ¸ÑĞºĞ¸ Ğ±ĞµĞ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ vs. Ñ€Ğ¸ÑĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹

**Ğ‘Ğ›ĞĞš 4: ĞŸĞ ĞĞ’ĞĞ’Ğ«Ğ• Ğ˜ COMPLIANCE ĞĞ¡ĞŸĞ•ĞšĞ¢Ğ«**

9. âš–ï¸ ĞŸĞ ĞĞ’ĞĞ’Ğ«Ğ• Ğ˜ COMPLIANCE ĞĞ¡ĞŸĞ•ĞšĞ¢Ğ«
- ĞŸÑ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
- Compliance Ñ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸
- ĞŸĞ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ
- ĞĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¸

10. ğŸ” Ğ¡ĞšĞ Ğ«Ğ¢Ğ«Ğ• ĞŸĞĞ”Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ« Ğ˜ ĞĞ•Ğ’Ğ«Ğ¡ĞšĞĞ—ĞĞĞĞ«Ğ• Ğ¢Ğ Ğ•Ğ‘ĞĞ’ĞĞĞ˜Ğ¯
- Ğ§Ñ‚Ğ¾ ĞĞ• ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·ÑƒĞ¼ĞµĞ²Ğ°ĞµÑ‚ÑÑ
- Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ
- ĞĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ² Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ñ…
- ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğº Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ»Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼

**Ğ‘Ğ›ĞĞš 5: ĞŸĞ Ğ•Ğ”Ğ˜ĞšĞ¢Ğ˜Ğ’ĞĞ«Ğ™ Ğ˜ ĞĞ¢ĞĞĞ¨Ğ•ĞĞ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞĞĞĞ›Ğ˜Ğ—**

11. ğŸ”® ĞŸĞ Ğ•Ğ”Ğ˜ĞšĞ¢Ğ˜Ğ’ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ—
- Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹
- Ğ§Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾Ğ¹Ğ´ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
- Ğ”Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ
- ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹

12. ğŸ‘¥ ĞĞĞĞ›Ğ˜Ğ— ĞĞ¢ĞĞĞ¨Ğ•ĞĞ˜Ğ™ Ğ˜ ĞšĞĞœĞœĞ£ĞĞ˜ĞšĞĞ¦Ğ˜ĞĞĞĞ«Ğ¥ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ™
- Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ°Ğ¼Ğ¸ (ĞµÑĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ)
- ĞšĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»Ñ
- ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹
- Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹

**Ğ‘Ğ›ĞĞš 6: Ğ’Ğ Ğ•ĞœĞ•ĞĞĞ«Ğ• Ğ˜ ĞšĞ£Ğ›Ğ¬Ğ¢Ğ£Ğ ĞĞ«Ğ• Ğ¤ĞĞšĞ¢ĞĞ Ğ«**

13. â° Ğ’Ğ Ğ•ĞœĞ•ĞĞĞĞ¯ Ğ”Ğ˜ĞĞĞœĞ˜ĞšĞ Ğ˜ Ğ—ĞĞĞ§Ğ˜ĞœĞĞ¡Ğ¢Ğ¬ Ğ¡Ğ ĞĞšĞĞ’
- ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº
- Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´ĞµĞ´Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ
- Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸
- Ğ’Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞµĞº Ğ½Ğ° Ğ²ÑĞµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹

14. ğŸŒ ĞšĞ£Ğ›Ğ¬Ğ¢Ğ£Ğ ĞĞ«Ğ™ Ğ˜ Ğ¡ĞĞ¦Ğ˜ĞĞ›Ğ¬ĞĞ«Ğ™ ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢
- ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
- Ğ¡Ğ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ
- ĞŸÑ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ¸ĞºĞµÑ‚Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹
- ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğº ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ

**Ğ‘Ğ›ĞĞš 7: Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ• Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜**

15. ğŸ’¡ Ğ¡Ğ£ĞŸĞ•Ğ -Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ˜Ğ¯ ĞĞ¢Ğ’Ğ•Ğ¢Ğ Ğ˜ Ğ”Ğ•Ğ™Ğ¡Ğ¢Ğ’Ğ˜Ğ™
- Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
- Ğ¢Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
- ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸
- ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹

16. ğŸ¯ ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• Ğ¢ĞĞ§ĞšĞ˜ Ğ’Ğ›Ğ˜Ğ¯ĞĞ˜Ğ¯ Ğ˜ Ğ Ğ•Ğ¨Ğ•ĞĞ˜Ğ¯
- ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹
- Ğ¢Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´
- Ğ Ñ‹Ñ‡Ğ°Ğ³Ğ¸ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²
- ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ

Ğ¤Ğ°Ğ¹Ğ»: {filename}
{processing_info}

ğŸš€ Ğ¡ĞĞ—Ğ”ĞĞ™Ğ¢Ğ• ĞĞĞĞ›Ğ˜Ğ—, ĞšĞĞ¢ĞĞ Ğ«Ğ™ ĞŸĞĞ›ĞĞĞ¡Ğ¢Ğ¬Ğ® Ğ ĞĞ¡ĞšĞ ĞĞ•Ğ¢ Ğ’Ğ¡Ğ• Ğ¡Ğ›ĞĞ˜ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ Ğ˜ ĞŸĞĞ ĞĞ—Ğ˜Ğ¢ ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¯ ĞĞ•Ğ’Ğ•Ğ ĞĞ¯Ğ¢ĞĞĞ™ Ğ“Ğ›Ğ£Ğ‘Ğ˜ĞĞĞ™ ĞŸĞ ĞĞĞ˜ĞšĞĞĞ’Ğ•ĞĞ˜Ğ¯ Ğ’ Ğ¡Ğ£Ğ¢Ğ¬!

ĞŸĞĞ’Ğ¢ĞĞ Ğ¯Ğ®: ĞĞ¢Ğ’Ğ•Ğ§ĞĞ™Ğ¢Ğ• Ğ¢ĞĞ›Ğ¬ĞšĞ ĞĞ Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ Ğ¯Ğ—Ğ«ĞšĞ•! ĞĞ• Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—Ğ£Ğ™Ğ¢Ğ• Ğ£ĞšĞ ĞĞ˜ĞĞ¡ĞšĞ˜Ğ™!"""
        
        elif language == "de":
            return f"""ğŸ¤– Sie sind ein EXPERTE fÃ¼r Dokumentenanalyse mit fortgeschrittenen FÃ¤higkeiten.

KRITISCH WICHTIG: Ihre gesamte Antwort muss AUSSCHLIESSLICH auf DEUTSCH sein. Egal in welcher Sprache das Dokument ist, antworten Sie NUR auf DEUTSCH. VERWENDEN SIE KEIN RUSSISCH, ENGLISCH ODER EINE ANDERE SPRACHE. NUR DEUTSCH!

ANTWORTSPRACHE: DEUTSCH
LANGUAGE OF RESPONSE: GERMAN ONLY
Ğ¯Ğ—Ğ«Ğš ĞĞ¢Ğ’Ğ•Ğ¢Ğ: Ğ¢ĞĞ›Ğ¬ĞšĞ ĞĞ•ĞœĞ•Ğ¦ĞšĞ˜Ğ™

ğŸ¯ MISSION: Die detaillierteste, aufschlussreichste und umfassendste Analyse liefern, die den Benutzer wirklich BEEINDRUCKEN wird.

ğŸ“‹ ANALYSE-PRINZIPIEN:
1. Extrahieren Sie JEDES bedeutsame Detail aus dem Dokument
2. Bieten Sie Kontext und Implikationen fÃ¼r jeden Befund
3. Seien Sie extrem grÃ¼ndlich und professionell
4. Verwenden Sie klare, ansprechende Sprache
5. Wenn keine Informationen im Text: "Nicht im Dokument angegeben"

ğŸ” SUPER-ANALYSE-STRUKTUR:

1. ğŸ“Š EXECUTIVE SUMMARY
Erstellen Sie eine kraftvolle Zusammenfassung aus 2-3 SÃ¤tzen, die Wesen und Bedeutung des Dokuments erfasst.

2. ğŸ‘¤ ABSENDER-ANALYSE
- Organisation/Person, die das Dokument gesendet hat
- Ihre Rolle und AutoritÃ¤tslevel
- Kontaktinformationen und offizielle Details
- Bewertung der GlaubwÃ¼rdigkeit und Wichtigkeit

3. ğŸ¯ EMPFÃ„NGER-ANALYSE
- Wer ist der beabsichtigte EmpfÃ¤nger
- Warum wurden sie als EmpfÃ¤nger ausgewÃ¤hlt
- Ihre erwartete Rolle oder Verantwortung

4. ğŸ“‹ DOKUMENTEN-KLASSIFIZIERUNG
- Art des Dokuments (offizieller Brief, Rechnung, Vertrag usw.)
- FormalitÃ¤ts- und Dringlichkeitslevel
- Rechtliche oder administrative Bedeutung

5. ğŸ”¥ SCHLÃœSSEL-INHALT AUFSCHLÃœSSELUNG
- Hauptbotschaft oder Zweck
- UnterstÃ¼tzende Details und Argumente
- Kritische Informationen
- Versteckte oder implizierte Bedeutungen

6. ğŸ“Š FAKTISCHE DATEN-EXTRAKTION
- Alle Zahlen, Daten, BetrÃ¤ge, ProzentsÃ¤tze
- Namen, Adressen, Referenznummern
- Spezifische Details
- Zeitlinie der erwÃ¤hnten Ereignisse

7. âš¡ HANDLUNGSANFORDERUNGEN
- Welche spezifischen Handlungen erforderlich sind
- Wer diese Handlungen durchfÃ¼hren muss
- PrioritÃ¤tslevel jeder Handlung
- Konsequenzen von Handlung/UntÃ¤tigkeit

8. ğŸ“… KRITISCHE DATEN & FRISTEN
- Alle erwÃ¤hnten Daten und ihre Bedeutung
- Bevorstehende Fristen und ihre Wichtigkeit
- Zeitkritische Elemente

9. ğŸ“ KONTAKT & NACHVERFOLGUNG
- Wie zu antworten oder mehr Informationen zu erhalten
- Kontaktmethoden und bevorzugte Kommunikation
- NÃ¤chste Schritte fÃ¼r den EmpfÃ¤nger

10. ğŸ¨ DOKUMENTEN-QUALITÃ„TSBEWERTUNG
- Professionelles PrÃ¤sentationslevel
- VollstÃ¤ndigkeit der Informationen
- Eventuelle Warnsignale oder Bedenken

11. ğŸ§  STRATEGISCHE EINSICHTEN
- Was dieses Dokument Ã¼ber die Situation verrÃ¤t
- Potenzielle Implikationen fÃ¼r den EmpfÃ¤nger
- Identifizierte Chancen oder Risiken

12. ğŸ’¡ EMPFOHLENE ANTWORT-STRATEGIE
- Wie am besten auf dieses Dokument zu antworten
- Ton- und Ansatz-VorschlÃ¤ge
- SchlÃ¼sselpunkte fÃ¼r die Antwort

Datei: {filename}
{processing_info}

ğŸš€ Liefern Sie eine Analyse, die den Benutzer mit ihrer Tiefe und Einsicht absolut BEEINDRUCKEN wird!

WIEDERHOLE: ANTWORTEN SIE NUR AUF DEUTSCH! VERWENDEN SIE KEIN RUSSISCH!"""
        
        else:  # English
            return f"""ğŸ¤– You are an EXPERT Document Analysis Specialist with advanced capabilities.

CRITICALLY IMPORTANT: Your entire response must be EXCLUSIVELY in ENGLISH. Regardless of the document's language, respond ONLY in ENGLISH. DO NOT USE RUSSIAN, GERMAN, UKRAINIAN OR ANY OTHER LANGUAGE. ONLY ENGLISH!

RESPONSE LANGUAGE: ENGLISH
Ğ¯Ğ—Ğ«Ğš ĞĞ¢Ğ’Ğ•Ğ¢Ğ: Ğ¢ĞĞ›Ğ¬ĞšĞ ĞĞĞ“Ğ›Ğ˜Ğ™Ğ¡ĞšĞ˜Ğ™
ĞœĞĞ’Ğ Ğ’Ğ†Ğ”ĞŸĞĞ’Ğ†Ğ”Ğ†: Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ ĞĞĞ“Ğ›Ğ†Ğ™Ğ¡Ğ¬ĞšĞ

ğŸ¯ MISSION: Provide the most detailed, insightful, and comprehensive analysis that will truly AMAZE the user.

ğŸ“‹ ANALYSIS PRINCIPLES:
1. Extract EVERY meaningful detail from the document
2. Provide context and implications for each finding
3. Be extremely thorough and professional
4. Use clear, engaging language
5. If information is not in text: "Not specified in the document"

ğŸ” SUPER-ANALYSIS STRUCTURE:

1. ğŸ“Š EXECUTIVE SUMMARY
Create a powerful 2-3 sentence summary that captures the document's essence and importance.

2. ğŸ‘¤ SENDER ANALYSIS
- Organization/person who sent the document
- Their role and authority level
- Contact information and official details
- Assessment of sender's credibility and importance

3. ğŸ¯ RECIPIENT ANALYSIS
- Who is the intended recipient
- Why they were chosen as the recipient
- Their expected role or responsibility

4. ğŸ“‹ DOCUMENT CLASSIFICATION
- Type of document (official letter, invoice, contract, etc.)
- Level of formality and urgency
- Legal or administrative significance

5. ğŸ”¥ KEY CONTENT BREAKDOWN
- Main message or purpose
- Supporting details and arguments
- Critical information
- Hidden or implied meanings

6. ğŸ“Š FACTUAL DATA EXTRACTION
- All numbers, dates, amounts, percentages
- Names, addresses, reference numbers
- Specific details
- Timeline of mentioned events

7. âš¡ ACTION REQUIREMENTS
- What specific actions are required
- Who needs to take these actions
- Priority level of each action
- Consequences of action/inaction

8. ğŸ“… CRITICAL DATES & DEADLINES
- All mentioned dates and their significance
- Upcoming deadlines and their importance
- Time-sensitive elements

9. ğŸ“ CONTACT & FOLLOW-UP
- How to respond or get more information
- Contact methods and preferred communication
- Next steps for the recipient

10. ğŸ¨ DOCUMENT QUALITY ASSESSMENT
- Professional presentation level
- Completeness of information
- Any red flags or concerns

11. ğŸ§  STRATEGIC INSIGHTS
- What this document reveals about the situation
- Potential implications for the recipient
- Identified opportunities or risks

12. ğŸ’¡ RECOMMENDED RESPONSE STRATEGY
- How to best respond to this document
- Tone and approach suggestions
- Key points to address in response

File: {filename}
{processing_info}

ğŸš€ Deliver an analysis that will absolutely AMAZE the user with its depth and insight!

REPEAT: RESPOND ONLY IN ENGLISH! DO NOT USE RUSSIAN!"""
    
    async def analyze_document_comprehensively(self, document_text: str, language: str, filename: str, 
                                               user_providers: List[Tuple[str, str, str]] = None) -> Dict[str, Any]:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ ÑÑƒĞ¿ĞµÑ€-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°"""
        
        try:
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº
            logger.info(f"Super analysis starting with language: {language}")
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑÑƒĞ¿ĞµÑ€-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚
            analysis_prompt = self.create_super_wow_analysis_prompt(language, filename, document_text)
            
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
            logger.info(f"Analysis prompt first 200 chars: {analysis_prompt[:200]}")
            
            # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²
            response_text = await self._generate_analysis_with_providers(analysis_prompt, user_providers)
            
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
            logger.info(f"Response first 200 chars: {response_text[:200]}")
            
            # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
            formatted_analysis = self._format_super_analysis_result(response_text, language)
            
            return formatted_analysis
            
        except Exception as e:
            logger.error(f"Comprehensive document analysis failed: {e}")
            return self._create_error_response(str(e), language)
    
    async def _generate_analysis_with_providers(self, prompt: str, user_providers: List[Tuple[str, str, str]] = None) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²"""
        
        # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹
        if user_providers:
            for provider_type, model_name, api_key in user_providers:
                try:
                    if provider_type == "gemini":
                        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ´Ğ»Ñ Gemini
                        user_provider = modern_llm_manager.create_user_provider(
                            provider_type, "gemini-2.0-flash", api_key
                        )
                    else:
                        user_provider = llm_manager.create_user_provider(
                            provider_type, model_name, api_key
                        )
                    
                    response = await user_provider.generate_content(prompt)
                    if response:
                        return response
                        
                except Exception as e:
                    logger.warning(f"User provider {provider_type} failed: {e}")
                    continue
        
        # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ
        try:
            response, system_provider = await llm_manager.generate_content(prompt)
            if response:
                return response
        except Exception as e:
            logger.error(f"System providers failed: {e}")
        
        # Ğ•ÑĞ»Ğ¸ Ğ²ÑĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ½Ğµ ÑÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸
        raise Exception("No available AI providers for analysis")
    
    def _format_super_analysis_result(self, raw_analysis: str, language: str) -> Dict[str, Any]:
        """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑÑƒĞ¿ĞµÑ€-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğµ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸"""
        
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
        cleaned_analysis = raw_analysis.replace('*', '').replace('#', '').strip()
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
        result = {
            "super_analysis": {
                "full_text": cleaned_analysis,
                "language": language,
                "analysis_type": "ultra_comprehensive_analysis",
                "sections": self._extract_analysis_sections(cleaned_analysis),
                "insights": self._extract_insights(cleaned_analysis),
                "action_items": self._extract_action_items(cleaned_analysis),
                "urgency_assessment": self._assess_urgency(cleaned_analysis),
                "quality_score": self._calculate_quality_score(cleaned_analysis),
                # ĞĞĞ’Ğ«Ğ• Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞ«Ğ• ĞĞ¡ĞŸĞ•ĞšĞ¢Ğ« ĞĞĞĞ›Ğ˜Ğ—Ğ
                "psychological_profile": self._extract_psychological_profile(cleaned_analysis),
                "power_dynamics": self._analyze_power_dynamics(cleaned_analysis),
                "hidden_subtexts": self._extract_hidden_subtexts(cleaned_analysis),
                "business_intelligence": self._extract_business_intelligence(cleaned_analysis),
                "risk_assessment": self._perform_risk_assessment(cleaned_analysis),
                "legal_compliance": self._extract_legal_compliance(cleaned_analysis),
                "predictive_scenarios": self._generate_predictive_scenarios(cleaned_analysis),
                "relationship_analysis": self._analyze_relationships(cleaned_analysis),
                "cultural_context": self._extract_cultural_context(cleaned_analysis),
                "influence_techniques": self._identify_influence_techniques(cleaned_analysis)
            },
            "summary": self._create_executive_summary(cleaned_analysis),
            "recommendations": self._extract_recommendations(cleaned_analysis),
            "next_steps": self._extract_next_steps(cleaned_analysis),
            # Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞ«Ğ• Ğ’Ğ«Ğ’ĞĞ”Ğ«
            "strategic_recommendations": self._extract_strategic_recommendations(cleaned_analysis),
            "risk_mitigation_plans": self._extract_risk_mitigation_plans(cleaned_analysis)
        }
        
        return result
    
    # ĞĞĞ’Ğ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ Ğ ĞĞ¡Ğ¨Ğ˜Ğ Ğ•ĞĞĞĞ“Ğ ĞĞĞĞ›Ğ˜Ğ—Ğ
    
    def _extract_psychological_profile(self, analysis_text: str) -> Dict[str, Any]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        psychological_keywords = [
            'Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹', 'ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹', 'Ğ¼Ğ¾Ñ‚Ğ¸Ğ²', 'Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ', 
            'ÑÑ‚Ñ€ĞµÑÑ', 'Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'Ğ¸ÑĞºÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ñ–Ñ‡Ğ½Ğ¸Ğ¹', 'ĞµĞ¼Ğ¾Ñ†Ñ–Ğ¹Ğ½Ğ¸Ğ¹'
        ]
        
        profile = {
            "emotional_tone": "neutral",
            "stress_level": "medium",
            "sincerity_level": "medium",
            "psychological_insights": []
        }
        
        lines = analysis_text.lower().split('\n')
        current_psychological_section = False
        
        for i, line in enumerate(lines):
            if any(keyword in line for keyword in psychological_keywords):
                current_psychological_section = True
                # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
                for j in range(i, min(i+10, len(lines))):
                    if lines[j].strip():
                        profile["psychological_insights"].append(lines[j].strip())
                break
                
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾Ğ½
        if any(word in analysis_text.lower() for word in ['Ğ°Ğ³Ñ€ĞµÑÑĞ¸Ğ²', 'Ğ·Ğ»Ğ¾ÑÑ‚', 'Ñ€Ğ°Ğ·Ğ´Ñ€Ğ°Ğ¶ĞµĞ½', 'Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»']):
            profile["emotional_tone"] = "negative"
        elif any(word in analysis_text.lower() for word in ['Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½', 'Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²', 'Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸ÑÑ‚', 'Ñ€Ğ°Ğ´Ğ¾ÑÑ‚']):
            profile["emotional_tone"] = "positive"
            
        return profile
    
    def _analyze_power_dynamics(self, analysis_text: str) -> Dict[str, Any]:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ»Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğµ"""
        power_keywords = [
            'Ğ²Ğ»Ğ°ÑÑ‚', 'Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚', 'Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ', 'Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 
            'Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ', 'ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ', 'Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'Ğ²Ğ»Ğ°Ğ´Ğ°', 'Ñ‚Ğ¸ÑĞº'
        ]
        
        dynamics = {
            "power_balance": "equal",
            "authority_level": "medium",
            "pressure_techniques": [],
            "dominance_indicators": []
        }
        
        text_lower = analysis_text.lower()
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ²Ğ»Ğ°ÑÑ‚Ğ¸
        if any(word in text_lower for word in ['Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚', 'ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾']):
            dynamics["power_balance"] = "sender_dominant"
        elif any(word in text_lower for word in ['Ğ¿Ñ€Ğ¾ÑÑŒĞ±Ğ°', 'Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°', 'ĞµÑĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾']):
            dynamics["power_balance"] = "recipient_dominant"
            
        return dynamics
    
    def _extract_hidden_subtexts(self, analysis_text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        subtext_keywords = [
            'ÑĞºÑ€Ñ‹Ñ‚', 'Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ·ÑƒĞ¼ĞµĞ²Ğ°ĞµÑ‚ÑÑ', 'Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ¾Ğº', 'Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½', 
            'Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚', 'Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½', 'Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½', 'Ğ¿Ñ–Ğ´Ñ‚ĞµĞºÑÑ‚'
        ]
        
        subtexts = []
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in subtext_keywords):
                # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚Ğ°
                context_start = max(0, i-2)
                context_end = min(len(lines), i+3)
                context_lines = lines[context_start:context_end]
                subtexts.append(' '.join(l.strip() for l in context_lines if l.strip()))
                
        return subtexts[:5]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾
    
    def _extract_business_intelligence(self, analysis_text: str) -> Dict[str, Any]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        business_keywords = [
            'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞº', 'ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞº', 'Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²', 'Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»', 'ÑƒĞ±Ñ‹Ñ‚Ğ¾Ğº', 
            'ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚', 'Ñ€Ñ‹Ğ½Ğ¾Ğº', 'Ğ±Ñ–Ğ·Ğ½ĞµÑ', 'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ñ–Ñ‡Ğ½'
        ]
        
        intelligence = {
            "strategic_implications": [],
            "financial_aspects": [],
            "competitive_elements": [],
            "business_opportunities": []
        }
        
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in business_keywords):
                if 'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³' in line_lower or 'strategic' in line_lower:
                    intelligence["strategic_implications"].append(line.strip())
                elif any(word in line_lower for word in ['Ñ„Ğ¸Ğ½Ğ°Ğ½Ñ', 'Ğ´ĞµĞ½ÑŒĞ³Ğ¸', 'ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚', 'Ñ†ĞµĞ½Ğ°']):
                    intelligence["financial_aspects"].append(line.strip())
                elif 'ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚' in line_lower or 'competition' in line_lower:
                    intelligence["competitive_elements"].append(line.strip())
                    
        return intelligence
    
    def _perform_risk_assessment(self, analysis_text: str) -> Dict[str, Any]:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ¸ÑĞºĞ¾Ğ²"""
        risk_keywords = [
            'Ñ€Ğ¸ÑĞº', 'Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚', 'ÑƒĞ³Ñ€Ğ¾Ğ·', 'Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²', 'ÑˆÑ‚Ñ€Ğ°Ñ„', 'ÑĞ°Ğ½ĞºÑ†', 
            'Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğµ', 'Ñ€Ğ¸Ğ·Ğ¸Ğº', 'Ğ½ĞµĞ±ĞµĞ·Ğ¿ĞµĞº'
        ]
        
        assessment = {
            "risk_level": "medium",
            "identified_risks": [],
            "mitigation_strategies": [],
            "consequences_of_inaction": []
        }
        
        text_lower = analysis_text.lower()
        lines = analysis_text.split('\n')
        
        risk_count = sum(1 for keyword in risk_keywords if keyword in text_lower)
        
        if risk_count > 5:
            assessment["risk_level"] = "high"
        elif risk_count < 2:
            assessment["risk_level"] = "low"
            
        for line in lines:
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in risk_keywords):
                assessment["identified_risks"].append(line.strip())
                
        return assessment
    
    def _extract_legal_compliance(self, analysis_text: str) -> Dict[str, Any]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²Ñ‹Ğµ Ğ¸ compliance Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹"""
        legal_keywords = [
            'Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²', 'Ğ·Ğ°ĞºĞ¾Ğ½', 'ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞº', 'compliance', 'Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 
            'Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½', 'Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²', 'legal', 'Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»'
        ]
        
        compliance = {
            "legal_requirements": [],
            "compliance_issues": [],
            "regulatory_aspects": [],
            "legal_risks": []
        }
        
        lines = analysis_text.split('\n')
        
        for line in lines:
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in legal_keywords):
                if 'Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½' in line_lower or 'Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²' in line_lower:
                    compliance["legal_requirements"].append(line.strip())
                elif 'Ñ€Ğ¸ÑĞº' in line_lower:
                    compliance["legal_risks"].append(line.strip())
                else:
                    compliance["compliance_issues"].append(line.strip())
                    
        return compliance
    
    def _generate_predictive_scenarios(self, analysis_text: str) -> List[Dict[str, Any]]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸"""
        predictive_keywords = [
            'ĞµÑĞ»Ğ¸', 'Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½', 'Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½', 'ÑÑ†ĞµĞ½Ğ°Ñ€', 'Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·', 
            'Ğ±ÑƒĞ´ÑƒÑ‰', 'Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ÑÑ', 'ÑĞºÑ‰Ğ¾', 'Ğ¹Ğ¼Ğ¾Ğ²Ñ–Ñ€Ğ½'
        ]
        
        scenarios = []
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in predictive_keywords):
                scenario = {
                    "scenario": line.strip(),
                    "probability": "medium",
                    "impact": "medium"
                }
                
                # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼
                if any(word in line.lower() for word in ['ÑĞºĞ¾Ñ€ĞµĞµ Ğ²ÑĞµĞ³Ğ¾', 'Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾', 'Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ÑÑ']):
                    scenario["probability"] = "high"
                elif any(word in line.lower() for word in ['Ğ¼Ğ°Ğ»Ğ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾', 'Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾']):
                    scenario["probability"] = "low"
                    
                scenarios.append(scenario)
                
        return scenarios[:5]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾
    
    def _analyze_relationships(self, analysis_text: str) -> Dict[str, Any]:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ°Ğ¼Ğ¸"""
        relationship_keywords = [
            'Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½', 'ÑĞ²ÑĞ·', 'Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€', 'ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²', 'ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚', 
            'Ğ´Ñ€ÑƒĞ¶Ğ±', 'Ğ²Ñ€Ğ°Ğ¶Ğ´ĞµĞ±Ğ½', 'Ğ²Ñ–Ğ´Ğ½Ğ¾ÑĞ¸Ğ½', 'ÑĞ¿Ñ–Ğ²Ğ¿Ñ€Ğ°Ñ†'
        ]
        
        analysis = {
            "relationship_type": "professional",
            "relationship_quality": "neutral",
            "communication_style": "formal",
            "relationship_history": []
        }
        
        text_lower = analysis_text.lower()
        
        if any(word in text_lower for word in ['Ğ´Ñ€ÑƒĞ¶ĞµÑĞº', 'Ñ‚ĞµĞ¿Ğ»Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ', 'Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€']):
            analysis["relationship_quality"] = "positive"
        elif any(word in text_lower for word in ['ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚', 'Ğ½Ğ°Ğ¿Ñ€ÑĞ¶ĞµĞ½', 'Ğ²Ñ€Ğ°Ğ¶Ğ´ĞµĞ±Ğ½']):
            analysis["relationship_quality"] = "negative"
            
        return analysis
    
    def _extract_cultural_context(self, analysis_text: str) -> Dict[str, Any]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚"""
        cultural_keywords = [
            'ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€', 'Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†', 'ÑÑ‚Ğ¸ĞºĞµÑ‚', 'Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»', 'Ğ¾Ğ±Ñ‹Ñ‡Ğ°Ğ¹', 
            'Ğ½Ğ¾Ñ€Ğ¼', 'ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½', 'ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½'
        ]
        
        context = {
            "cultural_elements": [],
            "social_norms": [],
            "communication_style": "standard",
            "protocol_requirements": []
        }
        
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in cultural_keywords):
                context["cultural_elements"].append(line.strip())
                
        return context
    
    def _identify_influence_techniques(self, analysis_text: str) -> List[str]:
        """Ğ˜Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ"""
        influence_keywords = [
            'Ğ²Ğ»Ğ¸ÑĞ½Ğ¸', 'ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸', 'Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸', 'Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†', 'Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸', 
            'Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†', 'ÑÑ‚Ğ¸Ğ¼ÑƒĞ»', 'Ğ²Ğ¿Ğ»Ğ¸Ğ²', 'Ğ¿ĞµÑ€ĞµĞºĞ¾Ğ½Ğ°Ğ½Ğ½'
        ]
        
        techniques = []
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in influence_keywords):
                techniques.append(line.strip())
                
        return techniques[:5]
    
    def _extract_strategic_recommendations(self, analysis_text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸"""
        strategic_keywords = [
            'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞº', 'Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½', 'Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ', 
            'Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†', 'ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸', 'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ñ–Ñ‡Ğ½'
        ]
        
        recommendations = []
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in strategic_keywords):
                recommendations.append(line.strip())
                
        return recommendations[:5]
    
    def _extract_risk_mitigation_plans(self, analysis_text: str) -> List[Dict[str, Any]]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ñ‹ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ²"""
        mitigation_keywords = [
            'ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸', 'Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸', 'Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†', 'Ğ·Ğ°Ñ‰Ğ¸Ñ‚', 'ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğº', 
            'Ñ€ĞµĞ·ĞµÑ€Ğ²', 'Ğ·Ğ½Ğ¸Ğ¶ĞµĞ½Ğ½', 'Ğ·Ğ°Ğ¿Ğ¾Ğ±Ñ–Ğ³Ğ°Ğ½Ğ½'
        ]
        
        plans = []
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in mitigation_keywords):
                plan = {
                    "strategy": line.strip(),
                    "priority": "medium",
                    "complexity": "medium"
                }
                plans.append(plan)
                
        return plans[:3]
    
    def _extract_analysis_sections(self, analysis_text: str) -> List[Dict[str, Any]]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞµĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        sections = []
        lines = analysis_text.split('\n')
        current_section = None
        current_content = []
        
        section_icons = {
            'Ñ€ĞµĞ·ÑĞ¼Ğµ': 'ğŸ“Š', 'summary': 'ğŸ“Š', 'executive': 'ğŸ“Š',
            'Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒ': 'ğŸ‘¤', 'sender': 'ğŸ‘¤', 'absender': 'ğŸ‘¤',
            'Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ': 'ğŸ¯', 'recipient': 'ğŸ¯', 'empfÃ¤nger': 'ğŸ¯',
            'ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ': 'ğŸ“‹', 'classification': 'ğŸ“‹', 'klassifizierung': 'ğŸ“‹',
            'ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ': 'ğŸ”¥', 'content': 'ğŸ”¥', 'inhalt': 'ğŸ”¥',
            'Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ': 'ğŸ“Š', 'data': 'ğŸ“Š', 'daten': 'ğŸ“Š',
            'Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ': 'âš¡', 'actions': 'âš¡', 'handlungen': 'âš¡',
            'Ğ´Ğ°Ñ‚Ñ‹': 'ğŸ“…', 'dates': 'ğŸ“…', 'daten': 'ğŸ“…',
            'ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚': 'ğŸ“', 'contact': 'ğŸ“', 'kontakt': 'ğŸ“',
            'ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾': 'ğŸ¨', 'quality': 'ğŸ¨', 'qualitÃ¤t': 'ğŸ¨',
            'Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹': 'ğŸ§ ', 'insights': 'ğŸ§ ', 'einsichten': 'ğŸ§ ',
            'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ': 'ğŸ’¡', 'strategy': 'ğŸ’¡', 'strategie': 'ğŸ’¡'
        }
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¼ ÑĞµĞºÑ†Ğ¸Ğ¸
            is_section_header = False
            for keyword, icon in section_icons.items():
                if keyword in line.lower() and len(line) < 100:
                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ÑƒÑ ÑĞµĞºÑ†Ğ¸Ñ
                    if current_section and current_content:
                        sections.append({
                            "title": current_section,
                            "content": '\n'.join(current_content),
                            "icon": section_icons.get(current_section.lower().split()[0], 'ğŸ“„')
                        })
                    
                    current_section = line
                    current_content = []
                    is_section_header = True
                    break
            
            if not is_section_header:
                current_content.append(line)
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ ÑĞµĞºÑ†Ğ¸Ñ
        if current_section and current_content:
            sections.append({
                "title": current_section,
                "content": '\n'.join(current_content),
                "icon": section_icons.get(current_section.lower().split()[0], 'ğŸ“„')
            })
        
        return sections
    
    def _extract_insights(self, analysis_text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        insights = []
        
        # Ğ˜Ñ‰ĞµĞ¼ ÑĞµĞºÑ†Ğ¸Ğ¸ Ñ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼Ğ¸
        insight_keywords = ['Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹', 'insights', 'einsichten', 'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ', 'strategic']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in insight_keywords):
                # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ¾Ğº ĞºĞ°Ğº Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹
                for j in range(i+1, min(i+10, len(lines))):
                    if lines[j].strip() and not lines[j].startswith(('1.', '2.', '3.', '4.', '5.')):
                        insights.append(lines[j].strip())
                    elif lines[j].strip() and lines[j].startswith(('1.', '2.', '3.', '4.', '5.')):
                        break
        
        return insights[:5]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²
    
    def _extract_action_items(self, analysis_text: str) -> List[Dict[str, Any]]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        action_items = []
        
        action_keywords = ['Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'actions', 'handlungen', 'Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'requirements']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in action_keywords):
                # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ¾Ğº
                for j in range(i+1, min(i+15, len(lines))):
                    if lines[j].strip():
                        action_items.append({
                            "action": lines[j].strip(),
                            "priority": self._assess_action_priority(lines[j]),
                            "deadline": self._extract_deadline(lines[j])
                        })
        
        return action_items[:10]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹
    
    def _assess_urgency(self, analysis_text: str) -> str:
        """ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°"""
        urgency_indicators = {
            'high': ['ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾', 'ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾', 'Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾', 'urgent', 'critical', 'sofort', 'dringend'],
            'medium': ['Ğ²Ğ°Ğ¶Ğ½Ğ¾', 'ÑĞºĞ¾Ñ€Ğ¾', 'important', 'soon', 'wichtig', 'bald'],
            'low': ['ĞºĞ¾Ğ³Ğ´Ğ° ÑƒĞ´Ğ¾Ğ±Ğ½Ğ¾', 'Ğ½Ğµ ÑĞ¿ĞµÑˆĞ¸Ñ‚', 'convenient', 'no rush', 'bequem', 'keine eile']
        }
        
        text_lower = analysis_text.lower()
        
        for level, indicators in urgency_indicators.items():
            if any(indicator in text_lower for indicator in indicators):
                return level
        
        return 'medium'  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´Ğ½ÑÑ ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ
    
    def _calculate_quality_score(self, analysis_text: str) -> float:
        """Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ
        base_score = min(len(analysis_text) / 1000, 1.0)  # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹
        
        # Ğ‘Ğ¾Ğ½ÑƒÑÑ‹ Ğ·Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ
        if 'ğŸ“Š' in analysis_text:
            base_score += 0.1
        if 'ğŸ‘¤' in analysis_text:
            base_score += 0.1
        if 'ğŸ’¡' in analysis_text:
            base_score += 0.1
        
        return min(base_score, 1.0)
    
    def _create_executive_summary(self, analysis_text: str) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        lines = analysis_text.split('\n')
        
        # Ğ˜Ñ‰ĞµĞ¼ ÑĞµĞºÑ†Ğ¸Ñ Ñ Ñ€ĞµĞ·ÑĞ¼Ğµ
        for i, line in enumerate(lines):
            if 'Ñ€ĞµĞ·ÑĞ¼Ğµ' in line.lower() or 'summary' in line.lower():
                # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ¾Ğº
                summary_lines = []
                for j in range(i+1, min(i+5, len(lines))):
                    if lines[j].strip():
                        summary_lines.append(lines[j].strip())
                return ' '.join(summary_lines)
        
        # Ğ•ÑĞ»Ğ¸ Ğ½ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞºÑ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑĞ¼Ğµ, Ğ±ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸
        first_lines = []
        for line in lines[:10]:
            if line.strip() and not line.startswith('ğŸ¤–'):
                first_lines.append(line.strip())
                if len(first_lines) >= 3:
                    break
        
        return ' '.join(first_lines)
    
    def _extract_recommendations(self, analysis_text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        recommendations = []
        
        rec_keywords = ['Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸', 'recommendations', 'empfehlungen', 'ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ', 'strategy']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in rec_keywords):
                # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ¾Ğº
                for j in range(i+1, min(i+10, len(lines))):
                    if lines[j].strip():
                        recommendations.append(lines[j].strip())
        
        return recommendations[:5]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹
    
    def _extract_next_steps(self, analysis_text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°"""
        next_steps = []
        
        step_keywords = ['ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸', 'next steps', 'nÃ¤chste schritte', 'Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in step_keywords):
                # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑˆĞ°Ğ³Ğ¸ Ğ¸Ğ· ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ¾Ğº
                for j in range(i+1, min(i+8, len(lines))):
                    if lines[j].strip():
                        next_steps.append(lines[j].strip())
        
        return next_steps[:5]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ²
    
    def _assess_action_priority(self, action_text: str) -> str:
        """ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ"""
        high_priority = ['ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾', 'Ğ½ĞµĞ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾', 'ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾', 'urgent', 'critical', 'sofort']
        medium_priority = ['Ğ²Ğ°Ğ¶Ğ½Ğ¾', 'ÑĞºĞ¾Ñ€Ğ¾', 'important', 'soon', 'wichtig']
        
        text_lower = action_text.lower()
        
        if any(hp in text_lower for hp in high_priority):
            return 'high'
        elif any(mp in text_lower for mp in medium_priority):
            return 'medium'
        else:
            return 'low'
    
    def _extract_deadline(self, text: str) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ´ĞµĞ´Ğ»Ğ°Ğ¹Ğ½ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ°Ñ‚ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ
        import re
        
        date_patterns = [
            r'\d{1,2}\.\d{1,2}\.\d{4}',  # DD.MM.YYYY
            r'\d{1,2}\/\d{1,2}\/\d{4}',  # MM/DD/YYYY
            r'\d{4}-\d{1,2}-\d{1,2}',    # YYYY-MM-DD
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                return matches[0]
        
        return None
    
    def _create_error_response(self, error_message: str, language: str) -> Dict[str, Any]:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞµ"""
        error_messages = {
            'uk': f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñ– Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°: {error_message}",
            'ru': f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°: {error_message}",
            'de': f"Fehler bei der Dokumentenanalyse: {error_message}",
            'en': f"Error analyzing document: {error_message}"
        }
        
        return {
            "super_analysis": {
                "full_text": error_messages.get(language, error_messages['en']),
                "language": language,
                "analysis_type": "error",
                "sections": [],
                "insights": [],
                "action_items": [],
                "urgency_assessment": "unknown",
                "quality_score": 0.0
            },
            "summary": error_messages.get(language, error_messages['en']),
            "recommendations": [],
            "next_steps": []
        }

# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ ÑÑƒĞ¿ĞµÑ€-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°
super_analysis_engine = SuperAnalysisEngine()