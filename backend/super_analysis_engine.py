"""
–ú–æ–¥—É–ª—å —Å—É–ø–µ—Ä-–∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è WOW-—ç—Ñ—Ñ–µ–∫—Ç–∞
–°–æ–∑–¥–∞–µ—Ç –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from llm_manager import llm_manager
from modern_llm_manager import modern_llm_manager

logger = logging.getLogger(__name__)

class SuperAnalysisEngine:
    """–î–≤–∏–∂–æ–∫ —Å—É–ø–µ—Ä-–∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è WOW-—ç—Ñ—Ñ–µ–∫—Ç–∞ —Å –≥–ª—É–±–æ–∫–∏–º –ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ–º –≤ —Å–∫—Ä—ã—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã"""
    
    def __init__(self):
        self.supported_languages = ['uk', 'ru', 'de', 'en']
        self.analysis_categories = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            'executive_summary',
            'sender_analysis', 
            'recipient_analysis',
            'document_classification',
            'key_content_breakdown',
            'factual_data_extraction',
            'action_requirements',
            'critical_dates',
            'contact_followup',
            'quality_assessment',
            'strategic_insights',
            'response_strategy',
            
            # –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò –î–õ–Ø –ì–õ–£–ë–û–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê
            'psychological_analysis',      # –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ—Ç–∏–≤–æ–≤ –∏ —ç–º–æ—Ü–∏–π
            'power_dynamics_analysis',     # –ê–Ω–∞–ª–∏–∑ –≤–ª–∞—Å—Ç–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π
            'hidden_subtexts',            # –°–∫—Ä—ã—Ç—ã–µ –ø–æ–¥—Ç–µ–∫—Å—Ç—ã –∏ –Ω–µ–≤—ã—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            'business_intelligence',       # –ë–∏–∑–Ω–µ—Å-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã
            'risk_assessment',            # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
            'legal_compliance',           # –ü—Ä–∞–≤–æ–≤—ã–µ –∏ compliance –∞—Å–ø–µ–∫—Ç—ã
            'relationship_analysis',       # –ê–Ω–∞–ª–∏–∑ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏
            'predictive_insights',        # –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏
            'emotional_intelligence',     # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            'cultural_context',           # –ö—É–ª—å—Ç—É—Ä–Ω—ã–π –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            'timing_significance',        # –ó–Ω–∞—á–∏–º–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å—Ä–æ–∫–æ–≤
            'communication_strategy',     # –ê–Ω–∞–ª–∏–∑ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            'influence_techniques',       # –¢–µ—Ö–Ω–∏–∫–∏ –≤–ª–∏—è–Ω–∏—è –∏ —É–±–µ–∂–¥–µ–Ω–∏—è
            'decision_pressure_points'    # –¢–æ—á–∫–∏ –¥–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
        ]
    
    def create_super_wow_analysis_prompt(self, language: str, filename: str, extracted_text: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å—É–ø–µ—Ä-–¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è WOW-–∞–Ω–∞–ª–∏–∑–∞"""
        
        processing_info = f"\n\nüìÑ –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ô –¢–ï–ö–°–¢ –ò–ó –î–û–ö–£–ú–ï–ù–¢–ê:\n{extracted_text}\n\n"
        
        if language == "uk":
            return f"""ü§ñ –í–∏ - –°–£–ü–ï–†-–ï–ö–°–ü–ï–†–¢ –Ü–Ü-–ê–ù–ê–õ–Ü–¢–ò–ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ –ø–µ—Ä–µ–¥–æ–≤–∏–º–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏ –≥–ª–∏–±–æ–∫–æ–≥–æ –ø—Ä–æ–Ω–∏–∫–Ω–µ–Ω–Ω—è –≤ –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ –∞—Å–ø–µ–∫—Ç–∏ –±—É–¥—å-—è–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.

–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–õ–ò–í–û: –í—Å—è –≤–∞—à–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ –í–ò–ö–õ–Æ–ß–ù–û –£–ö–†–ê–á–ù–°–¨–ö–û–Æ –º–æ–≤–æ—é. –ù–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –º–æ–≤–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ –¢–Ü–õ–¨–ö–ò –£–ö–†–ê–á–ù–°–¨–ö–û–Æ. –ù–ï –í–ò–ö–û–†–ò–°–¢–û–í–£–ô–¢–ï –†–û–°–Ü–ô–°–¨–ö–£, –ê–ù–ì–õ–Ü–ô–°–¨–ö–£ –ß–ò –ë–£–î–¨-–Ø–ö–£ –Ü–ù–®–£ –ú–û–í–£. –¢–Ü–õ–¨–ö–ò –£–ö–†–ê–á–ù–°–¨–ö–ê!

–ú–û–í–ê –í–Ü–î–ü–û–í–Ü–î–Ü: –£–ö–†–ê–á–ù–°–¨–ö–ê
LANGUAGE OF RESPONSE: UKRAINIAN ONLY
–Ø–ó–´–ö –û–¢–í–ï–¢–ê: –¢–û–õ–¨–ö–û –£–ö–†–ê–ò–ù–°–ö–ò–ô

üéØ –°–£–ü–ï–†-–ú–Ü–°–Ü–Ø: –ù–∞–¥–∞—Ç–∏ –ù–ï–ô–ú–û–í–Ü–†–ù–û –¥–µ—Ç–∞–ª—å–Ω–∏–π, –ø—Ä–æ–Ω–∏–∫–ª–∏–≤–∏–π —Ç–∞ –±–∞–≥–∞—Ç–æ—à–∞—Ä–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —è–∫–∏–π –≤–∫–ª—é—á–∞—î –í–°–Ü –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ –∞—Å–ø–µ–∫—Ç–∏, –º–æ—Ç–∏–≤–∏, —Ä–∏–∑–∏–∫–∏ —Ç–∞ –ø—ñ–¥—Ç–µ–∫—Å—Ç–∏. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –º–∞—î –±—É—Ç–∏ –í–†–ê–ñ–ï–ù–ò–ô –≥–ª–∏–±–∏–Ω–æ—é –∞–Ω–∞–ª—ñ–∑—É!

üìã –†–û–ó–®–ò–†–ï–ù–Ü –ü–†–ò–ù–¶–ò–ü–ò –ê–ù–ê–õ–Ü–ó–£:
1. –í–∏—Ç—è–≥—É–π—Ç–µ –ö–û–ñ–ù–£ –¥–µ—Ç–∞–ª—å —ñ —á–∏—Ç–∞–π—Ç–µ –º—ñ–∂ —Ä—è–¥–∫—ñ–≤
2. –ê–Ω–∞–ª—ñ–∑—É–π—Ç–µ –ø—Å–∏—Ö–æ–ª–æ–≥—ñ—á–Ω—ñ –º–æ—Ç–∏–≤–∏ —Ç–∞ –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ –Ω–∞–º—ñ—Ä–∏
3. –í–∏—è–≤–ª—è–π—Ç–µ –≤—Å—ñ —Ä–∏–∑–∏–∫–∏, –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Ç–∞ –ø—Ä–∞–≤–æ–≤—ñ –∞—Å–ø–µ–∫—Ç–∏
4. –†–æ–∑–∫—Ä–∏–≤–∞–π—Ç–µ –≤–ª–∞–¥–Ω—ñ –≤—ñ–¥–Ω–æ—Å–∏–Ω–∏ —Ç–∞ –¥–∏–Ω–∞–º—ñ–∫—É –≤–ø–ª–∏–≤—É
5. –ü–µ—Ä–µ–¥–±–∞—á–∞–π—Ç–µ –º–æ–∂–ª–∏–≤—ñ —Å—Ü–µ–Ω–∞—Ä—ñ—ó —Ä–æ–∑–≤–∏—Ç–∫—É –ø–æ–¥—ñ–π
6. –Ø–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–º–∞—î –≤ —Ç–µ–∫—Å—Ç—ñ: "–ù–µ –≤–∫–∞–∑–∞–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ"

üîç –°–£–ü–ï–†-–î–ï–¢–ê–õ–¨–ù–ê –°–¢–†–£–ö–¢–£–†–ê –ê–ù–ê–õ–Ü–ó–£ –ó –ì–õ–ò–ë–û–ö–ò–ú –ü–†–û–ù–ò–ö–ù–ï–ù–ù–Ø–ú:

**–ë–õ–û–ö 1: –û–°–ù–û–í–ù–ò–ô –ê–ù–ê–õ–Ü–ó**

1. üìä –í–ò–ö–û–ù–ê–í–ß–ï –†–ï–ó–Æ–ú–ï
–ü–æ—Ç—É–∂–Ω–µ —Ä–µ–∑—é–º–µ –∑ 3-4 —Ä–µ—á–µ–Ω—å, —â–æ —Ä–æ–∑–∫—Ä–∏–≤–∞—î —Å—É—Ç—å, –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å —Ç–∞ –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ –∞—Å–ø–µ–∫—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

2. üë§ –ì–õ–ò–ë–û–ö–ò–ô –ê–ù–ê–õ–Ü–ó –í–Ü–î–ü–†–ê–í–ù–ò–ö–ê
- –û—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è/–æ—Å–æ–±–∞: –ø–æ–≤–Ω–∞ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–∞ —Å—Ç–∞—Ç—É—Å
- –†—ñ–≤–µ–Ω—å –≤–ª–∞–¥–∏ —Ç–∞ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç—É –≤ —ñ—î—Ä–∞—Ä—Ö—ñ—ó
- –ú–æ—Ç–∏–≤–∏ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è —Ü—å–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ü—Å–∏—Ö–æ–ª–æ–≥—ñ—á–Ω–∏–π –ø—Ä–æ—Ñ—ñ–ª—å –∑–∞ —Å—Ç–∏–ª–µ–º –ª–∏—Å—Ç–∞
- –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —ñ–Ω—Ç–µ—Ä–µ—Å–∏ —Ç–∞ agenda

3. üéØ –ê–ù–ê–õ–Ü–ó –û–î–ï–†–ñ–£–í–ê–ß–ê –¢–ê –¶–Ü–õ–¨–û–í–û–á –ê–£–î–ò–¢–û–†–Ü–á
- –¶—ñ–ª—å–æ–≤–∞ –∞—É–¥–∏—Ç–æ—Ä—ñ—è —Ç–∞ –ø—Ä–∏—á–∏–Ω–∏ –≤–∏–±–æ—Ä—É
- –û—á—ñ–∫—É–≤–∞–Ω–∞ —Ä–µ–∞–∫—Ü—ñ—è –æ–¥–µ—Ä–∂—É–≤–∞—á–∞
- –í–ª–∞–¥–Ω—ñ –≤—ñ–¥–Ω–æ—Å–∏–Ω–∏ –º—ñ–∂ –≤—ñ–¥–ø—Ä–∞–≤–Ω–∏–∫–æ–º —Ç–∞ –æ–¥–µ—Ä–∂—É–≤–∞—á–µ–º
- –í–ø–ª–∏–≤ –Ω–∞ —Ä–µ–ø—É—Ç–∞—Ü—ñ—é —Ç–∞ –ø–æ–∑–∏—Ü—ñ—é –æ–¥–µ—Ä–∂—É–≤–∞—á–∞

4. üìã –†–û–ó–®–ò–†–ï–ù–ê –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø –î–û–ö–£–ú–ï–ù–¢–ê
- –î–µ—Ç–∞–ª—å–Ω–∞ —Ç–∏–ø–æ–ª–æ–≥—ñ—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –Æ—Ä–∏–¥–∏—á–Ω–∞ –∑–Ω–∞—á—É—â—ñ—Å—Ç—å —Ç–∞ –ø—Ä–∞–≤–æ–≤–∞ –≤–∞–≥–∞
- –ú—ñ—Å—Ü–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±—ñ–≥—É —Ç–∞ –±—ñ–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å–∞—Ö
- –†—ñ–≤–µ–Ω—å –∫–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω–æ—Å—Ç—ñ —Ç–∞ —Ä–∏–∑–∏–∫–∏ –≤–∏—Ç–æ–∫—É

**–ë–õ–û–ö 2: –ü–°–ò–•–û–õ–û–ì–Ü–ß–ù–ò–ô –¢–ê –ú–û–¢–ò–í–ê–¶–Ü–ô–ù–ò–ô –ê–ù–ê–õ–Ü–ó**

5. üß† –ü–°–ò–•–û–õ–û–ì–Ü–ß–ù–ò–ô –ê–ù–ê–õ–Ü–ó
- –ï–º–æ—Ü—ñ–π–Ω–∏–π —Ç–æ–Ω —Ç–∞ –Ω–∞—Å—Ç—Ä—ñ–π –≤—ñ–¥–ø—Ä–∞–≤–Ω–∏–∫–∞
- –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ –º–æ—Ç–∏–≤–∏ —Ç–∞ —Å–ø—Ä–∞–≤–∂–Ω—ñ –Ω–∞–º—ñ—Ä–∏
- –¢–µ—Ö–Ω—ñ–∫–∏ –≤–ø–ª–∏–≤—É —Ç–∞ –º–∞–Ω—ñ–ø—É–ª—é–≤–∞–Ω–Ω—è (—è–∫—â–æ —î)
- –†—ñ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—É –∞–±–æ —Ç–∏—Å–∫—É –Ω–∞ –≤—ñ–¥–ø—Ä–∞–≤–Ω–∏–∫–∞
- –©–∏—Ä—ñ—Å—Ç—å vs. —Ñ–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è

6. üíº –ê–ù–ê–õ–Ü–ó –í–õ–ê–î–ù–ò–• –í–Ü–î–ù–û–°–ò–ù
- –•—Ç–æ –º–∞—î –±—ñ–ª—å—à–µ –≤–ª–∞–¥–∏ –≤ –¥–∞–Ω—ñ–π —Å–∏—Ç—É–∞—Ü—ñ—ó
- –¢–µ—Ö–Ω—ñ–∫–∏ —Ç–∏—Å–∫—É —Ç–∞ –ø—Ä–∏–º—É—à–µ–Ω–Ω—è
- –°–ø–æ—Å–æ–±–∏ –ø—Ä–æ—è–≤—É –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç—É –≤ —Ç–µ–∫—Å—Ç—ñ
- –ë–∞–ª–∞–Ω—Å —Å–∏–ª —Ç–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –¥–ª—è –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ñ–≤

–§–∞–π–ª: {filename}
{processing_info}

üöÄ –°–¢–í–û–†–Ü–¢–¨ –ê–ù–ê–õ–Ü–ó, –Ø–ö–ò–ô –ü–û–í–ù–Ü–°–¢–Æ –†–û–ó–ö–†–ò–Ñ –í–°–Ü –®–ê–†–ò –î–û–ö–£–ú–ï–ù–¢–ê –¢–ê –í–†–ê–ó–ò–¢–¨ –ö–û–†–ò–°–¢–£–í–ê–ß–ê –ù–ï–ô–ú–û–í–Ü–†–ù–û–Æ –ì–õ–ò–ë–ò–ù–û–Æ –ü–†–û–ù–ò–ö–ù–ï–ù–ù–Ø –í –°–£–¢–¨!

–ü–û–í–¢–û–†–Æ–Æ: –í–Ü–î–ü–û–í–Ü–î–ê–ô–¢–ï –¢–Ü–õ–¨–ö–ò –£–ö–†–ê–á–ù–°–¨–ö–û–Æ –ú–û–í–û–Æ! –ù–ï –í–ò–ö–û–†–ò–°–¢–û–í–£–ô–¢–ï –†–û–°–Ü–ô–°–¨–ö–£!"""
        
        elif language == "ru":
            return f"""ü§ñ –í—ã - –°–£–ü–ï–†-–≠–ö–°–ü–ï–†–¢ –ò–ò-–ê–ù–ê–õ–ò–¢–ò–ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã –ª—é–±—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–ù–û: –í–µ—Å—å –≤–∞—à –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ. –ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —è–∑—ã–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –æ—Ç–≤–µ—á–∞–π—Ç–µ –¢–û–õ–¨–ö–û –Ω–∞ –†–£–°–°–ö–û–ú. –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –£–ö–†–ê–ò–ù–°–ö–ò–ô, –ê–ù–ì–õ–ò–ô–°–ö–ò–ô –ò–õ–ò –õ–Æ–ë–û–ô –î–†–£–ì–û–ô –Ø–ó–´–ö. –¢–û–õ–¨–ö–û –†–£–°–°–ö–ò–ô!

–Ø–ó–´–ö –û–¢–í–ï–¢–ê: –†–£–°–°–ö–ò–ô
LANGUAGE OF RESPONSE: RUSSIAN ONLY
–ú–û–í–ê –í–Ü–î–ü–û–í–Ü–î–Ü: –¢–Ü–õ–¨–ö–ò –†–û–°–Ü–ô–°–¨–ö–ê

üéØ –°–£–ü–ï–†-–ú–ò–°–°–ò–Ø: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –ù–ï–í–ï–†–û–Ø–¢–ù–û –¥–µ—Ç–∞–ª—å–Ω—ã–π, –ø—Ä–æ–Ω–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤–∫–ª—é—á–∞–µ—Ç –í–°–ï —Å–∫—Ä—ã—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã, –º–æ—Ç–∏–≤—ã, —Ä–∏—Å–∫–∏ –∏ –ø–æ–¥—Ç–µ–∫—Å—Ç—ã. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ü–û–†–ê–ñ–ï–ù –≥–ª—É–±–∏–Ω–æ–π –∞–Ω–∞–ª–∏–∑–∞!

üìã –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ü–†–ò–ù–¶–ò–ü–´ –ê–ù–ê–õ–ò–ó–ê:
1. –ò–∑–≤–ª–µ–∫–∞–π—Ç–µ –ö–ê–ñ–î–£–Æ –¥–µ—Ç–∞–ª—å –∏ —á–∏—Ç–∞–π—Ç–µ –º–µ–∂–¥—É —Å—Ç—Ä–æ–∫
2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –º–æ—Ç–∏–≤—ã –∏ —Å–∫—Ä—ã—Ç—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
3. –í—ã—è–≤–ª—è–π—Ç–µ –≤—Å–µ —Ä–∏—Å–∫–∏, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∞–≤–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã
4. –†–∞—Å–∫—Ä—ã–≤–∞–π—Ç–µ –≤–ª–∞—Å—Ç–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∏ –¥–∏–Ω–∞–º–∏–∫—É –≤–ª–∏—è–Ω–∏—è
5. –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–π—Ç–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–æ–±—ã—Ç–∏–π
6. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ: "–ù–µ —É–∫–∞–∑–∞–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"

üîç –°–£–ü–ï–†-–î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê –ê–ù–ê–õ–ò–ó–ê –° –ì–õ–£–ë–û–ö–ò–ú –ü–†–û–ù–ò–ö–ù–û–í–ï–ù–ò–ï–ú:

**–ë–õ–û–ö 1: –û–°–ù–û–í–ù–û–ô –ê–ù–ê–õ–ò–ó**

1. üìä –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ï –†–ï–ó–Æ–ú–ï
–ú–æ—â–Ω–æ–µ —Ä–µ–∑—é–º–µ –∏–∑ 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–µ–µ —Å—É—Ç—å, –≤–∞–∂–Ω–æ—Å—Ç—å –∏ —Å–∫—Ä—ã—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞.

2. üë§ –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –û–¢–ü–†–ê–í–ò–¢–ï–õ–Ø
- –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è/–ª–∏—Ü–æ: –ø–æ–ª–Ω–∞—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Å—Ç–∞—Ç—É—Å
- –£—Ä–æ–≤–µ–Ω—å –≤–ª–∞—Å—Ç–∏ –∏ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–∞ –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏
- –ú–æ—Ç–∏–≤—ã –æ—Ç–ø—Ä–∞–≤–∫–∏ —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å –ø–æ —Å—Ç–∏–ª—é –ø–∏—Å—å–º–∞
- –°–∫—Ä—ã—Ç—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∏ agenda

3. üéØ –ê–ù–ê–õ–ò–ó –ü–û–õ–£–ß–ê–¢–ï–õ–Ø –ò –¶–ï–õ–ï–í–û–ô –ê–£–î–ò–¢–û–†–ò–ò
- –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è –∏ –ø—Ä–∏—á–∏–Ω—ã –≤—ã–±–æ—Ä–∞
- –û–∂–∏–¥–∞–µ–º–∞—è —Ä–µ–∞–∫—Ü–∏—è –ø–æ–ª—É—á–∞—Ç–µ–ª—è
- –í–ª–∞—Å—Ç–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª–µ–º –∏ –ø–æ–ª—É—á–∞—Ç–µ–ª–µ–º
- –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ–ø—É—Ç–∞—Ü–∏—é –∏ –ø–æ–∑–∏—Ü–∏—é –ø–æ–ª—É—á–∞—Ç–µ–ª—è

4. üìã –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –î–û–ö–£–ú–ï–ù–¢–ê
- –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∏–ø–æ–ª–æ–≥–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –∏ –ø—Ä–∞–≤–æ–≤–æ–π –≤–µ—Å
- –ú–µ—Å—Ç–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç–µ –∏ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å–∞—Ö
- –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ä–∏—Å–∫–∏ —É—Ç–µ—á–∫–∏

**–ë–õ–û–ö 2: –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ò –ú–û–¢–ò–í–ê–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó**

5. üß† –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è
- –°–∫—Ä—ã—Ç—ã–µ –º–æ—Ç–∏–≤—ã –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
- –¢–µ—Ö–Ω–∏–∫–∏ –≤–ª–∏—è–Ω–∏—è –∏ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞ –∏–ª–∏ –¥–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è
- –ò—Å–∫—Ä–µ–Ω–Ω–æ—Å—Ç—å vs. —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è

6. üíº –ê–ù–ê–õ–ò–ó –í–õ–ê–°–¢–ù–´–• –û–¢–ù–û–®–ï–ù–ò–ô
- –ö—Ç–æ –∏–º–µ–µ—Ç –±–æ–ª—å—à–µ –≤–ª–∞—Å—Ç–∏ –≤ –¥–∞–Ω–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏
- –¢–µ—Ö–Ω–∏–∫–∏ –¥–∞–≤–ª–µ–Ω–∏—è –∏ –ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏—è
- –°–ø–æ—Å–æ–±—ã –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ
- –ë–∞–ª–∞–Ω—Å —Å–∏–ª –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤

**–ë–õ–û–ö 3: –ë–ò–ó–ù–ï–°-–ò–ù–¢–ï–õ–õ–ï–ö–¢ –ò –°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó**

7. üìà –ë–ò–ó–ù–ï–°-–ò–ù–¢–ï–õ–õ–ï–ö–¢ –ê–ù–ê–õ–ò–ó
- –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è
- –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–æ—Ç–∏–≤—ã
- –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∏ —Ä—ã–Ω–æ—á–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏–ª–∏ –±–∏–∑–Ω–µ—Å–∞

8. ‚ö†Ô∏è –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ò–°–ö–û–í
- –ü—Ä–∞–≤–æ–≤—ã–µ —Ä–∏—Å–∫–∏ –¥–ª—è –ø–æ–ª—É—á–∞—Ç–µ–ª—è
- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∏ —Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏
- –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ —É–≥—Ä–æ–∑—ã –±–∏–∑–Ω–µ—Å—É
- –†–∏—Å–∫–∏ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏—è vs. —Ä–∏—Å–∫–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π

**–ë–õ–û–ö 4: –ü–†–ê–í–û–í–´–ï –ò COMPLIANCE –ê–°–ü–ï–ö–¢–´**

9. ‚öñÔ∏è –ü–†–ê–í–û–í–´–ï –ò COMPLIANCE –ê–°–ü–ï–ö–¢–´
- –ü—Ä–∞–≤–æ–≤—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- Compliance —Å —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏

10. üîç –°–ö–†–´–¢–´–ï –ü–û–î–¢–ï–ö–°–¢–´ –ò –ù–ï–í–´–°–ö–ê–ó–ê–ù–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø
- –ß—Ç–æ –ù–ï —Å–∫–∞–∑–∞–Ω–æ, –Ω–æ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç—Å—è
- –°–∫—Ä—ã—Ç—ã–µ —É—Å–ª–æ–≤–∏—è –∏ –æ–∂–∏–¥–∞–Ω–∏—è
- –ù–µ–≤–µ—Ä–±–∞–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö
- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –±—É–¥—É—â–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏—è–º

**–ë–õ–û–ö 5: –ü–†–ï–î–ò–ö–¢–ò–í–ù–´–ô –ò –û–¢–ù–û–®–ï–ù–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó**

11. üîÆ –ü–†–ï–î–ò–ö–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó
- –í–µ—Ä–æ—è—Ç–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–æ–±—ã—Ç–∏–π
- –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö –æ—Ç–≤–µ—Ç–∞
- –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –∏ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ—á–∫–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

12. üë• –ê–ù–ê–õ–ò–ó –û–¢–ù–û–®–ï–ù–ò–ô –ò –ö–û–ú–ú–£–ù–ò–ö–ê–¶–ò–û–ù–ù–´–• –°–¢–†–ê–¢–ï–ì–ò–ô
- –ò—Å—Ç–æ—Ä–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ (–µ—Å–ª–∏ –º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å)
- –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è
- –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –∏–ª–∏ —É—Ö—É–¥—à–µ–Ω–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π

**–ë–õ–û–ö 6: –í–†–ï–ú–ï–ù–ù–´–ï –ò –ö–£–õ–¨–¢–£–†–ù–´–ï –§–ê–ö–¢–û–†–´**

13. ‚è∞ –í–†–ï–ú–ï–ù–ù–ê–Ø –î–ò–ù–ê–ú–ò–ö–ê –ò –ó–ù–ê–ß–ò–ú–û–°–¢–¨ –°–†–û–ö–û–í
- –ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫
- –°–∫—Ä—ã—Ç—ã–µ –¥–µ–¥–ª–∞–π–Ω—ã –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏
- –í–ª–∏—è–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–µ–∫ –Ω–∞ –≤—Å–µ —Å—Ç–æ—Ä–æ–Ω—ã

14. üåç –ö–£–õ–¨–¢–£–†–ù–´–ô –ò –°–û–¶–ò–ê–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢
- –ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏
- –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –æ–∂–∏–¥–∞–Ω–∏—è
- –ü—Ä–æ—Ç–æ–∫–æ–ª—å–Ω—ã–µ –∏ —ç—Ç–∏–∫–µ—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
- –ê–¥–∞–ø—Ç–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –∫ –∫—É–ª—å—Ç—É—Ä–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É

**–ë–õ–û–ö 7: –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò**

15. üí° –°–£–ü–ï–†-–°–¢–†–ê–¢–ï–ì–ò–Ø –û–¢–í–ï–¢–ê –ò –î–ï–ô–°–¢–í–ò–ô
- –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
- –¢–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è
- –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
- –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏ –∑–∞–ø–∞—Å–Ω—ã–µ –ø–ª–∞–Ω—ã

16. üéØ –ö–õ–Æ–ß–ï–í–´–ï –¢–û–ß–ö–ò –í–õ–ò–Ø–ù–ò–Ø –ò –†–ï–®–ï–ù–ò–Ø
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
- –¢–æ—á–∫–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –∏—Å—Ö–æ–¥
- –†—ã—á–∞–≥–∏ –¥–∞–≤–ª–µ–Ω–∏—è –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤
- –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç–∞–π–º–∏–Ω–≥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è

–§–∞–π–ª: {filename}
{processing_info}

üöÄ –°–û–ó–î–ê–ô–¢–ï –ê–ù–ê–õ–ò–ó, –ö–û–¢–û–†–´–ô –ü–û–õ–ù–û–°–¢–¨–Æ –†–ê–°–ö–†–û–ï–¢ –í–°–ï –°–õ–û–ò –î–û–ö–£–ú–ï–ù–¢–ê –ò –ü–û–†–ê–ó–ò–¢ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø –ù–ï–í–ï–†–û–Ø–¢–ù–û–ô –ì–õ–£–ë–ò–ù–û–ô –ü–†–û–ù–ò–ö–ù–û–í–ï–ù–ò–Ø –í –°–£–¢–¨!

–ü–û–í–¢–û–†–Ø–Æ: –û–¢–í–ï–ß–ê–ô–¢–ï –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï! –ù–ï –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï –£–ö–†–ê–ò–ù–°–ö–ò–ô!"""
        
        elif language == "de":
            return f"""ü§ñ Sie sind ein EXPERTE f√ºr Dokumentenanalyse mit fortgeschrittenen F√§higkeiten.

KRITISCH WICHTIG: Ihre gesamte Antwort muss AUSSCHLIESSLICH auf DEUTSCH sein. Egal in welcher Sprache das Dokument ist, antworten Sie NUR auf DEUTSCH. VERWENDEN SIE KEIN RUSSISCH, ENGLISCH ODER EINE ANDERE SPRACHE. NUR DEUTSCH!

ANTWORTSPRACHE: DEUTSCH
LANGUAGE OF RESPONSE: GERMAN ONLY
–Ø–ó–´–ö –û–¢–í–ï–¢–ê: –¢–û–õ–¨–ö–û –ù–ï–ú–ï–¶–ö–ò–ô

üéØ MISSION: Die detaillierteste, aufschlussreichste und umfassendste Analyse liefern, die den Benutzer wirklich BEEINDRUCKEN wird.

üìã ANALYSE-PRINZIPIEN:
1. Extrahieren Sie JEDES bedeutsame Detail aus dem Dokument
2. Bieten Sie Kontext und Implikationen f√ºr jeden Befund
3. Seien Sie extrem gr√ºndlich und professionell
4. Verwenden Sie klare, ansprechende Sprache
5. Wenn keine Informationen im Text: "Nicht im Dokument angegeben"

üîç SUPER-ANALYSE-STRUKTUR:

1. üìä EXECUTIVE SUMMARY
Erstellen Sie eine kraftvolle Zusammenfassung aus 2-3 S√§tzen, die Wesen und Bedeutung des Dokuments erfasst.

2. üë§ ABSENDER-ANALYSE
- Organisation/Person, die das Dokument gesendet hat
- Ihre Rolle und Autorit√§tslevel
- Kontaktinformationen und offizielle Details
- Bewertung der Glaubw√ºrdigkeit und Wichtigkeit

3. üéØ EMPF√ÑNGER-ANALYSE
- Wer ist der beabsichtigte Empf√§nger
- Warum wurden sie als Empf√§nger ausgew√§hlt
- Ihre erwartete Rolle oder Verantwortung

4. üìã DOKUMENTEN-KLASSIFIZIERUNG
- Art des Dokuments (offizieller Brief, Rechnung, Vertrag usw.)
- Formalit√§ts- und Dringlichkeitslevel
- Rechtliche oder administrative Bedeutung

5. üî• SCHL√úSSEL-INHALT AUFSCHL√úSSELUNG
- Hauptbotschaft oder Zweck
- Unterst√ºtzende Details und Argumente
- Kritische Informationen
- Versteckte oder implizierte Bedeutungen

6. üìä FAKTISCHE DATEN-EXTRAKTION
- Alle Zahlen, Daten, Betr√§ge, Prozents√§tze
- Namen, Adressen, Referenznummern
- Spezifische Details
- Zeitlinie der erw√§hnten Ereignisse

7. ‚ö° HANDLUNGSANFORDERUNGEN
- Welche spezifischen Handlungen erforderlich sind
- Wer diese Handlungen durchf√ºhren muss
- Priorit√§tslevel jeder Handlung
- Konsequenzen von Handlung/Unt√§tigkeit

8. üìÖ KRITISCHE DATEN & FRISTEN
- Alle erw√§hnten Daten und ihre Bedeutung
- Bevorstehende Fristen und ihre Wichtigkeit
- Zeitkritische Elemente

9. üìû KONTAKT & NACHVERFOLGUNG
- Wie zu antworten oder mehr Informationen zu erhalten
- Kontaktmethoden und bevorzugte Kommunikation
- N√§chste Schritte f√ºr den Empf√§nger

10. üé® DOKUMENTEN-QUALIT√ÑTSBEWERTUNG
- Professionelles Pr√§sentationslevel
- Vollst√§ndigkeit der Informationen
- Eventuelle Warnsignale oder Bedenken

11. üß† STRATEGISCHE EINSICHTEN
- Was dieses Dokument √ºber die Situation verr√§t
- Potenzielle Implikationen f√ºr den Empf√§nger
- Identifizierte Chancen oder Risiken

12. üí° EMPFOHLENE ANTWORT-STRATEGIE
- Wie am besten auf dieses Dokument zu antworten
- Ton- und Ansatz-Vorschl√§ge
- Schl√ºsselpunkte f√ºr die Antwort

Datei: {filename}
{processing_info}

üöÄ Liefern Sie eine Analyse, die den Benutzer mit ihrer Tiefe und Einsicht absolut BEEINDRUCKEN wird!

WIEDERHOLE: ANTWORTEN SIE NUR AUF DEUTSCH! VERWENDEN SIE KEIN RUSSISCH!"""
        
        else:  # English
            return f"""ü§ñ You are an EXPERT Document Analysis Specialist with advanced capabilities.

CRITICALLY IMPORTANT: Your entire response must be EXCLUSIVELY in ENGLISH. Regardless of the document's language, respond ONLY in ENGLISH. DO NOT USE RUSSIAN, GERMAN, UKRAINIAN OR ANY OTHER LANGUAGE. ONLY ENGLISH!

RESPONSE LANGUAGE: ENGLISH
–Ø–ó–´–ö –û–¢–í–ï–¢–ê: –¢–û–õ–¨–ö–û –ê–ù–ì–õ–ò–ô–°–ö–ò–ô
–ú–û–í–ê –í–Ü–î–ü–û–í–Ü–î–Ü: –¢–Ü–õ–¨–ö–ò –ê–ù–ì–õ–Ü–ô–°–¨–ö–ê

üéØ MISSION: Provide the most detailed, insightful, and comprehensive analysis that will truly AMAZE the user.

üìã ANALYSIS PRINCIPLES:
1. Extract EVERY meaningful detail from the document
2. Provide context and implications for each finding
3. Be extremely thorough and professional
4. Use clear, engaging language
5. If information is not in text: "Not specified in the document"

üîç SUPER-ANALYSIS STRUCTURE:

1. üìä EXECUTIVE SUMMARY
Create a powerful 2-3 sentence summary that captures the document's essence and importance.

2. üë§ SENDER ANALYSIS
- Organization/person who sent the document
- Their role and authority level
- Contact information and official details
- Assessment of sender's credibility and importance

3. üéØ RECIPIENT ANALYSIS
- Who is the intended recipient
- Why they were chosen as the recipient
- Their expected role or responsibility

4. üìã DOCUMENT CLASSIFICATION
- Type of document (official letter, invoice, contract, etc.)
- Level of formality and urgency
- Legal or administrative significance

5. üî• KEY CONTENT BREAKDOWN
- Main message or purpose
- Supporting details and arguments
- Critical information
- Hidden or implied meanings

6. üìä FACTUAL DATA EXTRACTION
- All numbers, dates, amounts, percentages
- Names, addresses, reference numbers
- Specific details
- Timeline of mentioned events

7. ‚ö° ACTION REQUIREMENTS
- What specific actions are required
- Who needs to take these actions
- Priority level of each action
- Consequences of action/inaction

8. üìÖ CRITICAL DATES & DEADLINES
- All mentioned dates and their significance
- Upcoming deadlines and their importance
- Time-sensitive elements

9. üìû CONTACT & FOLLOW-UP
- How to respond or get more information
- Contact methods and preferred communication
- Next steps for the recipient

10. üé® DOCUMENT QUALITY ASSESSMENT
- Professional presentation level
- Completeness of information
- Any red flags or concerns

11. üß† STRATEGIC INSIGHTS
- What this document reveals about the situation
- Potential implications for the recipient
- Identified opportunities or risks

12. üí° RECOMMENDED RESPONSE STRATEGY
- How to best respond to this document
- Tone and approach suggestions
- Key points to address in response

File: {filename}
{processing_info}

üöÄ Deliver an analysis that will absolutely AMAZE the user with its depth and insight!

REPEAT: RESPOND ONLY IN ENGLISH! DO NOT USE RUSSIAN!"""
    
    async def analyze_document_comprehensively(self, document_text: str, language: str, filename: str, 
                                               user_providers: List[Tuple[str, str, str]] = None) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π —Å—É–ø–µ—Ä-–∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫
            logger.info(f"Super analysis starting with language: {language}")
            
            # –°–æ–∑–¥–∞–µ–º —Å—É–ø–µ—Ä-–ø—Ä–æ–º–ø—Ç
            analysis_prompt = self.create_super_wow_analysis_prompt(language, filename, document_text)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            logger.info(f"Analysis prompt first 200 chars: {analysis_prompt[:200]}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
            response_text = await self._generate_analysis_with_providers(analysis_prompt, user_providers)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞
            logger.info(f"Response first 200 chars: {response_text[:200]}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
            formatted_analysis = self._format_super_analysis_result(response_text, language)
            
            return formatted_analysis
            
        except Exception as e:
            logger.error(f"Comprehensive document analysis failed: {e}")
            return self._create_error_response(str(e), language)
    
    async def _generate_analysis_with_providers(self, prompt: str, user_providers: List[Tuple[str, str, str]] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        if user_providers:
            for provider_type, model_name, api_key in user_providers:
                try:
                    if provider_type == "gemini":
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è Gemini
                        user_provider = modern_llm_manager.create_user_provider(
                            provider_type, "gemini-2.0-flash", api_key
                        )
                    else:
                        user_provider = llm_manager.create_user_provider(
                            provider_type, model_name, api_key
                        )
                    
                    response = await user_provider.generate_content(prompt)
                    if response:
                        return response
                        
                except Exception as e:
                    logger.warning(f"User provider {provider_type} failed: {e}")
                    continue
        
        # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ
        try:
            response, system_provider = await llm_manager.generate_content(prompt)
            if response:
                return response
        except Exception as e:
            logger.error(f"System providers failed: {e}")
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        raise Exception("No available AI providers for analysis")
    
    def _format_super_analysis_result(self, raw_analysis: str, language: str) -> Dict[str, Any]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–ø–µ—Ä-–∞–Ω–∞–ª–∏–∑–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏"""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        cleaned_analysis = raw_analysis.replace('*', '').replace('#', '').strip()
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "super_analysis": {
                "full_text": cleaned_analysis,
                "language": language,
                "analysis_type": "ultra_comprehensive_analysis",
                "sections": self._extract_analysis_sections(cleaned_analysis),
                "insights": self._extract_insights(cleaned_analysis),
                "action_items": self._extract_action_items(cleaned_analysis),
                "urgency_assessment": self._assess_urgency(cleaned_analysis),
                "quality_score": self._calculate_quality_score(cleaned_analysis),
                # –ù–û–í–´–ï –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ê–°–ü–ï–ö–¢–´ –ê–ù–ê–õ–ò–ó–ê
                "psychological_profile": self._extract_psychological_profile(cleaned_analysis),
                "power_dynamics": self._analyze_power_dynamics(cleaned_analysis),
                "hidden_subtexts": self._extract_hidden_subtexts(cleaned_analysis),
                "business_intelligence": self._extract_business_intelligence(cleaned_analysis),
                "risk_assessment": self._perform_risk_assessment(cleaned_analysis),
                "legal_compliance": self._extract_legal_compliance(cleaned_analysis),
                "predictive_scenarios": self._generate_predictive_scenarios(cleaned_analysis),
                "relationship_analysis": self._analyze_relationships(cleaned_analysis),
                "cultural_context": self._extract_cultural_context(cleaned_analysis),
                "influence_techniques": self._identify_influence_techniques(cleaned_analysis)
            },
            "summary": self._create_executive_summary(cleaned_analysis),
            "recommendations": self._extract_recommendations(cleaned_analysis),
            "next_steps": self._extract_next_steps(cleaned_analysis),
            # –†–ê–°–®–ò–†–ï–ù–ù–´–ï –í–´–í–û–î–´
            "strategic_recommendations": self._extract_strategic_recommendations(cleaned_analysis),
            "risk_mitigation_plans": self._extract_risk_mitigation_plans(cleaned_analysis)
        }
        
        return result
    
    # –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê
    
    def _extract_psychological_profile(self, analysis_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        psychological_keywords = [
            '–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π', '—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π', '–º–æ—Ç–∏–≤', '–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ', 
            '—Å—Ç—Ä–µ—Å—Å', '–¥–∞–≤–ª–µ–Ω–∏–µ', '–∏—Å–∫—Ä–µ–Ω–Ω–æ—Å—Ç—å', '–ø—Å–∏—Ö–æ–ª–æ–≥—ñ—á–Ω–∏–π', '–µ–º–æ—Ü—ñ–π–Ω–∏–π'
        ]
        
        profile = {
            "emotional_tone": "neutral",
            "stress_level": "medium",
            "sincerity_level": "medium",
            "psychological_insights": []
        }
        
        lines = analysis_text.lower().split('\n')
        current_psychological_section = False
        
        for i, line in enumerate(lines):
            if any(keyword in line for keyword in psychological_keywords):
                current_psychological_section = True
                # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏
                for j in range(i, min(i+10, len(lines))):
                    if lines[j].strip():
                        profile["psychological_insights"].append(lines[j].strip())
                break
                
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω
        if any(word in analysis_text.lower() for word in ['–∞–≥—Ä–µ—Å—Å–∏–≤', '–∑–ª–æ—Å—Ç', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω', '–Ω–µ–¥–æ–≤–æ–ª']):
            profile["emotional_tone"] = "negative"
        elif any(word in analysis_text.lower() for word in ['–¥—Ä—É–∂–µ–ª—é–±–Ω', '–ø–æ–∑–∏—Ç–∏–≤', '–æ–ø—Ç–∏–º–∏—Å—Ç', '—Ä–∞–¥–æ—Å—Ç']):
            profile["emotional_tone"] = "positive"
            
        return profile
    
    def _analyze_power_dynamics(self, analysis_text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∞—Å—Ç–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        power_keywords = [
            '–≤–ª–∞—Å—Ç', '–∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç', '–∏–µ—Ä–∞—Ä—Ö–∏—è', '–¥–∞–≤–ª–µ–Ω–∏–µ', '–ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ', 
            '–≤–ª–∏—è–Ω–∏–µ', '–∫–æ–Ω—Ç—Ä–æ–ª—å', '–¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '–≤–ª–∞–¥–∞', '—Ç–∏—Å–∫'
        ]
        
        dynamics = {
            "power_balance": "equal",
            "authority_level": "medium",
            "pressure_techniques": [],
            "dominance_indicators": []
        }
        
        text_lower = analysis_text.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–ª–∞–Ω—Å –≤–ª–∞—Å—Ç–∏
        if any(word in text_lower for word in ['–≤—ã—Å–æ–∫–∏–π –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç', '—Å—Ç—Ä–æ–≥–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ']):
            dynamics["power_balance"] = "sender_dominant"
        elif any(word in text_lower for word in ['–ø—Ä–æ—Å—å–±–∞', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ']):
            dynamics["power_balance"] = "recipient_dominant"
            
        return dynamics
    
    def _extract_hidden_subtexts(self, analysis_text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø–æ–¥—Ç–µ–∫—Å—Ç—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        subtext_keywords = [
            '—Å–∫—Ä—ã—Ç', '–ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç—Å—è', '–º–µ–∂–¥—É —Å—Ç—Ä–æ–∫', '–Ω–µ–≤—ã—Å–∫–∞–∑–∞–Ω–Ω', 
            '–ø–æ–¥—Ç–µ–∫—Å—Ç', '–∏–º–ø–ª–∏—Ü–∏—Ç–Ω', '–ø—Ä–∏—Ö–æ–≤–∞–Ω', '–ø—ñ–¥—Ç–µ–∫—Å—Ç'
        ]
        
        subtexts = []
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in subtext_keywords):
                # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ç–µ–∫—Å—Ç–∞
                context_start = max(0, i-2)
                context_end = min(len(lines), i+3)
                context_lines = lines[context_start:context_end]
                subtexts.append(' '.join(l.strip() for l in context_lines if l.strip()))
                
        return subtexts[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _extract_business_intelligence(self, analysis_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∏–∑–Ω–µ—Å-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        business_keywords = [
            '—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–ø—Ä–∏–±—ã–ª', '—É–±—ã—Ç–æ–∫', 
            '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç', '—Ä—ã–Ω–æ–∫', '–±—ñ–∑–Ω–µ—Å', '—Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω'
        ]
        
        intelligence = {
            "strategic_implications": [],
            "financial_aspects": [],
            "competitive_elements": [],
            "business_opportunities": []
        }
        
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in business_keywords):
                if '—Å—Ç—Ä–∞—Ç–µ–≥' in line_lower or 'strategic' in line_lower:
                    intelligence["strategic_implications"].append(line.strip())
                elif any(word in line_lower for word in ['—Ñ–∏–Ω–∞–Ω—Å', '–¥–µ–Ω—å–≥–∏', '—Å—Ç–æ–∏–º–æ—Å—Ç', '—Ü–µ–Ω–∞']):
                    intelligence["financial_aspects"].append(line.strip())
                elif '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç' in line_lower or 'competition' in line_lower:
                    intelligence["competitive_elements"].append(line.strip())
                    
        return intelligence
    
    def _perform_risk_assessment(self, analysis_text: str) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ä–∏—Å–∫–æ–≤"""
        risk_keywords = [
            '—Ä–∏—Å–∫', '–æ–ø–∞—Å–Ω–æ—Å—Ç', '—É–≥—Ä–æ–∑', '–ø–æ—Å–ª–µ–¥—Å—Ç–≤', '—à—Ç—Ä–∞—Ñ', '—Å–∞–Ω–∫—Ü', 
            '–Ω–∞—Ä—É—à–µ–Ω–∏–µ', '—Ä–∏–∑–∏–∫', '–Ω–µ–±–µ–∑–ø–µ–∫'
        ]
        
        assessment = {
            "risk_level": "medium",
            "identified_risks": [],
            "mitigation_strategies": [],
            "consequences_of_inaction": []
        }
        
        text_lower = analysis_text.lower()
        lines = analysis_text.split('\n')
        
        risk_count = sum(1 for keyword in risk_keywords if keyword in text_lower)
        
        if risk_count > 5:
            assessment["risk_level"] = "high"
        elif risk_count < 2:
            assessment["risk_level"] = "low"
            
        for line in lines:
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in risk_keywords):
                assessment["identified_risks"].append(line.strip())
                
        return assessment
    
    def _extract_legal_compliance(self, analysis_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–æ–≤—ã–µ –∏ compliance –∞—Å–ø–µ–∫—Ç—ã"""
        legal_keywords = [
            '–ø—Ä–∞–≤–æ–≤', '–∑–∞–∫–æ–Ω', '—é—Ä–∏–¥–∏—á–µ—Å–∫', 'compliance', '—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', 
            '–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω', '–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤', 'legal', '–ø—Ä–∞–≤–∏–ª'
        ]
        
        compliance = {
            "legal_requirements": [],
            "compliance_issues": [],
            "regulatory_aspects": [],
            "legal_risks": []
        }
        
        lines = analysis_text.split('\n')
        
        for line in lines:
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in legal_keywords):
                if '—Ç—Ä–µ–±–æ–≤–∞–Ω' in line_lower or '–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤' in line_lower:
                    compliance["legal_requirements"].append(line.strip())
                elif '—Ä–∏—Å–∫' in line_lower:
                    compliance["legal_risks"].append(line.strip())
                else:
                    compliance["compliance_issues"].append(line.strip())
                    
        return compliance
    
    def _generate_predictive_scenarios(self, analysis_text: str) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏"""
        predictive_keywords = [
            '–µ—Å–ª–∏', '–≤–µ—Ä–æ—è—Ç–Ω', '–≤–æ–∑–º–æ–∂–Ω', '—Å—Ü–µ–Ω–∞—Ä', '–ø—Ä–æ–≥–Ω–æ–∑', 
            '–±—É–¥—É—â', '–æ–∂–∏–¥–∞–µ—Ç—Å—è', '—è–∫—â–æ', '–π–º–æ–≤—ñ—Ä–Ω'
        ]
        
        scenarios = []
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in predictive_keywords):
                scenario = {
                    "scenario": line.strip(),
                    "probability": "medium",
                    "impact": "medium"
                }
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                if any(word in line.lower() for word in ['—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ', '–≤–µ—Ä–æ—è—Ç–Ω–æ', '–æ–∂–∏–¥–∞–µ—Ç—Å—è']):
                    scenario["probability"] = "high"
                elif any(word in line.lower() for word in ['–º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ', '–≤–æ–∑–º–æ–∂–Ω–æ']):
                    scenario["probability"] = "low"
                    
                scenarios.append(scenario)
                
        return scenarios[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _analyze_relationships(self, analysis_text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏"""
        relationship_keywords = [
            '–æ—Ç–Ω–æ—à–µ–Ω', '—Å–≤—è–∑', '–ø–∞—Ä—Ç–Ω–µ—Ä', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', 
            '–¥—Ä—É–∂–±', '–≤—Ä–∞–∂–¥–µ–±–Ω', '–≤—ñ–¥–Ω–æ—Å–∏–Ω', '—Å–ø—ñ–≤–ø—Ä–∞—Ü'
        ]
        
        analysis = {
            "relationship_type": "professional",
            "relationship_quality": "neutral",
            "communication_style": "formal",
            "relationship_history": []
        }
        
        text_lower = analysis_text.lower()
        
        if any(word in text_lower for word in ['–¥—Ä—É–∂–µ—Å–∫', '—Ç–µ–ø–ª—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è', '–ø–∞—Ä—Ç–Ω–µ—Ä']):
            analysis["relationship_quality"] = "positive"
        elif any(word in text_lower for word in ['–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–Ω–∞–ø—Ä—è–∂–µ–Ω', '–≤—Ä–∞–∂–¥–µ–±–Ω']):
            analysis["relationship_quality"] = "negative"
            
        return analysis
    
    def _extract_cultural_context(self, analysis_text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫—É–ª—å—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        cultural_keywords = [
            '–∫—É–ª—å—Ç—É—Ä', '—Ç—Ä–∞–¥–∏—Ü', '—ç—Ç–∏–∫–µ—Ç', '–ø—Ä–æ—Ç–æ–∫–æ–ª', '–æ–±—ã—á–∞–π', 
            '–Ω–æ—Ä–º', '—Å–æ—Ü–∏–∞–ª—å–Ω', '–∫—É–ª—å—Ç—É—Ä–Ω'
        ]
        
        context = {
            "cultural_elements": [],
            "social_norms": [],
            "communication_style": "standard",
            "protocol_requirements": []
        }
        
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in cultural_keywords):
                context["cultural_elements"].append(line.strip())
                
        return context
    
    def _identify_influence_techniques(self, analysis_text: str) -> List[str]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏–∫–∏ –≤–ª–∏—è–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ"""
        influence_keywords = [
            '–≤–ª–∏—è–Ω–∏', '—É–±–µ–∂–¥–µ–Ω–∏', '–¥–∞–≤–ª–µ–Ω–∏', '–º–∞–Ω–∏–ø—É–ª—è—Ü', '–ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏', 
            '–º–æ—Ç–∏–≤–∞—Ü', '—Å—Ç–∏–º—É–ª', '–≤–ø–ª–∏–≤', '–ø–µ—Ä–µ–∫–æ–Ω–∞–Ω–Ω'
        ]
        
        techniques = []
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in influence_keywords):
                techniques.append(line.strip())
                
        return techniques[:5]
    
    def _extract_strategic_recommendations(self, analysis_text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        strategic_keywords = [
            '—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫', '–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ä–∞–∑–≤–∏—Ç–∏–µ', 
            '–æ–ø—Ç–∏–º–∏–∑–∞—Ü', '—É–ª—É—á—à–µ–Ω–∏', '—Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω'
        ]
        
        recommendations = []
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in strategic_keywords):
                recommendations.append(line.strip())
                
        return recommendations[:5]
    
    def _extract_risk_mitigation_plans(self, analysis_text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–∞–Ω—ã —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤"""
        mitigation_keywords = [
            '—Å–Ω–∏–∂–µ–Ω–∏', '–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏', '–º–∏–Ω–∏–º–∏–∑–∞—Ü', '–∑–∞—â–∏—Ç', '—Å—Ç—Ä–∞—Ö–æ–≤–∫', 
            '—Ä–µ–∑–µ—Ä–≤', '–∑–Ω–∏–∂–µ–Ω–Ω', '–∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω'
        ]
        
        plans = []
        lines = analysis_text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in mitigation_keywords):
                plan = {
                    "strategy": line.strip(),
                    "priority": "medium",
                    "complexity": "medium"
                }
                plans.append(plan)
                
        return plans[:3]
    
    def _extract_analysis_sections(self, analysis_text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–∫—Ü–∏–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        sections = []
        lines = analysis_text.split('\n')
        current_section = None
        current_content = []
        
        section_icons = {
            '—Ä–µ–∑—é–º–µ': 'üìä', 'summary': 'üìä', 'executive': 'üìä',
            '–æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å': 'üë§', 'sender': 'üë§', 'absender': 'üë§',
            '–ø–æ–ª—É—á–∞—Ç–µ–ª—å': 'üéØ', 'recipient': 'üéØ', 'empf√§nger': 'üéØ',
            '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è': 'üìã', 'classification': 'üìã', 'klassifizierung': 'üìã',
            '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ': 'üî•', 'content': 'üî•', 'inhalt': 'üî•',
            '–¥–∞–Ω–Ω—ã–µ': 'üìä', 'data': 'üìä', 'daten': 'üìä',
            '–¥–µ–π—Å—Ç–≤–∏—è': '‚ö°', 'actions': '‚ö°', 'handlungen': '‚ö°',
            '–¥–∞—Ç—ã': 'üìÖ', 'dates': 'üìÖ', 'daten': 'üìÖ',
            '–∫–æ–Ω—Ç–∞–∫—Ç': 'üìû', 'contact': 'üìû', 'kontakt': 'üìû',
            '–∫–∞—á–µ—Å—Ç–≤–æ': 'üé®', 'quality': 'üé®', 'qualit√§t': 'üé®',
            '–∏–Ω—Å–∞–π—Ç—ã': 'üß†', 'insights': 'üß†', 'einsichten': 'üß†',
            '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è': 'üí°', 'strategy': 'üí°', 'strategie': 'üí°'
        }
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Å–µ–∫—Ü–∏–∏
            is_section_header = False
            for keyword, icon in section_icons.items():
                if keyword in line.lower() and len(line) < 100:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                    if current_section and current_content:
                        sections.append({
                            "title": current_section,
                            "content": '\n'.join(current_content),
                            "icon": section_icons.get(current_section.lower().split()[0], 'üìÑ')
                        })
                    
                    current_section = line
                    current_content = []
                    is_section_header = True
                    break
            
            if not is_section_header:
                current_content.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_section and current_content:
            sections.append({
                "title": current_section,
                "content": '\n'.join(current_content),
                "icon": section_icons.get(current_section.lower().split()[0], 'üìÑ')
            })
        
        return sections
    
    def _extract_insights(self, analysis_text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        insights = []
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ —Å –∏–Ω—Å–∞–π—Ç–∞–º–∏
        insight_keywords = ['–∏–Ω—Å–∞–π—Ç—ã', 'insights', 'einsichten', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ', 'strategic']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in insight_keywords):
                # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –∫–∞–∫ –∏–Ω—Å–∞–π—Ç—ã
                for j in range(i+1, min(i+10, len(lines))):
                    if lines[j].strip() and not lines[j].startswith(('1.', '2.', '3.', '4.', '5.')):
                        insights.append(lines[j].strip())
                    elif lines[j].strip() and lines[j].startswith(('1.', '2.', '3.', '4.', '5.')):
                        break
        
        return insights[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Å–∞–π—Ç–æ–≤
    
    def _extract_action_items(self, analysis_text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        action_items = []
        
        action_keywords = ['–¥–µ–π—Å—Ç–≤–∏—è', 'actions', 'handlungen', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', 'requirements']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in action_keywords):
                # –°–æ–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫
                for j in range(i+1, min(i+15, len(lines))):
                    if lines[j].strip():
                        action_items.append({
                            "action": lines[j].strip(),
                            "priority": self._assess_action_priority(lines[j]),
                            "deadline": self._extract_deadline(lines[j])
                        })
        
        return action_items[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π
    
    def _assess_urgency(self, analysis_text: str) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Å—Ä–æ—á–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        urgency_indicators = {
            'high': ['—Å—Ä–æ—á–Ω–æ', '–∫—Ä–∏—Ç–∏—á–Ω–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', 'urgent', 'critical', 'sofort', 'dringend'],
            'medium': ['–≤–∞–∂–Ω–æ', '—Å–∫–æ—Ä–æ', 'important', 'soon', 'wichtig', 'bald'],
            'low': ['–∫–æ–≥–¥–∞ —É–¥–æ–±–Ω–æ', '–Ω–µ —Å–ø–µ—à–∏—Ç', 'convenient', 'no rush', 'bequem', 'keine eile']
        }
        
        text_lower = analysis_text.lower()
        
        for level, indicators in urgency_indicators.items():
            if any(indicator in text_lower for indicator in indicators):
                return level
        
        return 'medium'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—Ä–µ–¥–Ω—è—è —Å—Ä–æ—á–Ω–æ—Å—Ç—å
    
    def _calculate_quality_score(self, analysis_text: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        base_score = min(len(analysis_text) / 1000, 1.0)  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        if 'üìä' in analysis_text:
            base_score += 0.1
        if 'üë§' in analysis_text:
            base_score += 0.1
        if 'üí°' in analysis_text:
            base_score += 0.1
        
        return min(base_score, 1.0)
    
    def _create_executive_summary(self, analysis_text: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞"""
        lines = analysis_text.split('\n')
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏—é —Å —Ä–µ–∑—é–º–µ
        for i, line in enumerate(lines):
            if '—Ä–µ–∑—é–º–µ' in line.lower() or 'summary' in line.lower():
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫
                summary_lines = []
                for j in range(i+1, min(i+5, len(lines))):
                    if lines[j].strip():
                        summary_lines.append(lines[j].strip())
                return ' '.join(summary_lines)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Å–µ–∫—Ü–∏–∏ —Ä–µ–∑—é–º–µ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        first_lines = []
        for line in lines[:10]:
            if line.strip() and not line.startswith('ü§ñ'):
                first_lines.append(line.strip())
                if len(first_lines) >= 3:
                    break
        
        return ' '.join(first_lines)
    
    def _extract_recommendations(self, analysis_text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        recommendations = []
        
        rec_keywords = ['—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', 'recommendations', 'empfehlungen', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', 'strategy']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in rec_keywords):
                # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫
                for j in range(i+1, min(i+10, len(lines))):
                    if lines[j].strip():
                        recommendations.append(lines[j].strip())
        
        return recommendations[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    
    def _extract_next_steps(self, analysis_text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
        next_steps = []
        
        step_keywords = ['—Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏', 'next steps', 'n√§chste schritte', '–¥–∞–ª—å–Ω–µ–π—à–∏–µ –¥–µ–π—Å—Ç–≤–∏—è']
        lines = analysis_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(keyword in line.lower() for keyword in step_keywords):
                # –°–æ–±–∏—Ä–∞–µ–º —à–∞–≥–∏ –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫
                for j in range(i+1, min(i+8, len(lines))):
                    if lines[j].strip():
                        next_steps.append(lines[j].strip())
        
        return next_steps[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
    
    def _assess_action_priority(self, action_text: str) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è"""
        high_priority = ['—Å—Ä–æ—á–Ω–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '–∫—Ä–∏—Ç–∏—á–Ω–æ', 'urgent', 'critical', 'sofort']
        medium_priority = ['–≤–∞–∂–Ω–æ', '—Å–∫–æ—Ä–æ', 'important', 'soon', 'wichtig']
        
        text_lower = action_text.lower()
        
        if any(hp in text_lower for hp in high_priority):
            return 'high'
        elif any(mp in text_lower for mp in medium_priority):
            return 'medium'
        else:
            return 'low'
    
    def _extract_deadline(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ–¥–ª–∞–π–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –¥–∞—Ç –≤ —Ç–µ–∫—Å—Ç–µ
        import re
        
        date_patterns = [
            r'\d{1,2}\.\d{1,2}\.\d{4}',  # DD.MM.YYYY
            r'\d{1,2}\/\d{1,2}\/\d{4}',  # MM/DD/YYYY
            r'\d{4}-\d{1,2}-\d{1,2}',    # YYYY-MM-DD
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                return matches[0]
        
        return None
    
    def _create_error_response(self, error_message: str, language: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ–± –æ—à–∏–±–∫–µ"""
        error_messages = {
            'uk': f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª—ñ–∑—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {error_message}",
            'ru': f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {error_message}",
            'de': f"Fehler bei der Dokumentenanalyse: {error_message}",
            'en': f"Error analyzing document: {error_message}"
        }
        
        return {
            "super_analysis": {
                "full_text": error_messages.get(language, error_messages['en']),
                "language": language,
                "analysis_type": "error",
                "sections": [],
                "insights": [],
                "action_items": [],
                "urgency_assessment": "unknown",
                "quality_score": 0.0
            },
            "summary": error_messages.get(language, error_messages['en']),
            "recommendations": [],
            "next_steps": []
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å—É–ø–µ—Ä-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
super_analysis_engine = SuperAnalysisEngine()